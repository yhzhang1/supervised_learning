{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNLSTM, Dropout\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import huber_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给ret分类\n",
    "def label_ret(ret):\n",
    "    '''\n",
    "    class 0 = [-inf, -0.1]\n",
    "    class 1 = [-0.1, 0.1] unprofitable\n",
    "    class 2 = [0.1, inf]\n",
    "    '''\n",
    "    label = None\n",
    "    if ret < -0.1:\n",
    "        label = 0\n",
    "    elif -0.1 <= ret and ret <= 0.1:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 2\n",
    "        \n",
    "    return label\n",
    "def label_ret_bi(ret):\n",
    "    label = None\n",
    "    if ret <= 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "def label_ret2(ret):\n",
    "\n",
    "    if -0.1 <= ret and ret <= 0.1:\n",
    "        return 0\n",
    "    elif 0.1 < ret and ret <= 0.3:\n",
    "        return 1\n",
    "    elif 0.3 < ret:\n",
    "        return 2\n",
    "    elif -0.3 <= ret and ret < -0.1:\n",
    "        return 3\n",
    "    elif ret < -0.3:\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "        \n",
    "def generate_sequence(X_df, y_series, seq_length):\n",
    "    assert (X_df.index == y_series.index).all()\n",
    "    dataX = list()\n",
    "    dataY = list()\n",
    "    index = list()\n",
    "    for i in range(0, X_df.shape[0] - seq_length + 1):\n",
    "        dataX.append(X_df[i:i+seq_length])\n",
    "        dataY.append(y_series[i+seq_length-1])\n",
    "        index.append(y_series.index[i+seq_length-1])\n",
    "        \n",
    "    return dataX, dataY, pd.Index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety = 'RB'\n",
    "factor_store = pd.HDFStore('/home/data/vb/training_x_150.h5', mode='r')\n",
    "factor_df = factor_store.get(variety)\n",
    "y_store = pd.HDFStore('/home/data/vb/training_y_reg_150.h5', mode='r')\n",
    "y_series = y_store.get(variety)\n",
    "helper_df = pd.read_parquet('/home/data/vb/training_helper_150_{}.parquet'.format(variety))\n",
    "\n",
    "# 对ret做分类\n",
    "ret_y_series = np.exp(y_series) - 1 # 获得回报的原始收益\n",
    "\n",
    "label_y_series = ret_y_series.transform(label_ret2).rename('Y_label') # 分类标签\n",
    "ret_label_df = pd.concat([ret_y_series, label_y_series], axis=1) # 合并log ret和label\n",
    "assert (factor_df.index == label_y_series.index).all() # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# 对齐日期 去掉na\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "df = helper_df.join(factor_df, how='inner').join(label_y_series, how='inner')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 再次得到 factor_df, label_y_series, helper_df\n",
    "factor_df = df[factor_df.columns]\n",
    "label_y_series = df[label_y_series.name]\n",
    "helper_df = df[helper_df.columns]\n",
    "\n",
    "assert (factor_df.index == label_y_series.index).all() and \\\n",
    "        (label_y_series.index == helper_df.index).all()     # 确认数据和标签索引一样\n",
    "\n",
    "# train val test split\n",
    "factor_df_train, factor_df_test, label_y_series_train, label_y_series_test = \\\n",
    "train_test_split(factor_df, label_y_series, test_size=0.2, shuffle=False)\n",
    "factor_df_train, factor_df_val, label_y_series_train, label_y_series_val = \\\n",
    "train_test_split(factor_df_train, label_y_series_train, test_size=0.1, shuffle=False)\n",
    "\n",
    "\n",
    "# normalize data 在这里会丢失dataframe\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(factor_df_train)\n",
    "factor_train_normalized = scaler.transform(factor_df_train)\n",
    "factor_val_normalized = scaler.transform(factor_df_val)\n",
    "factor_test_normalized = scaler.transform(factor_df_test)\n",
    "\n",
    "\n",
    "# 将dataframe的index和columns加回去\n",
    "factor_df_train_normalized = pd.DataFrame(factor_train_normalized, \n",
    "                                          index=factor_df_train.index, columns=factor_df_train.columns)\n",
    "factor_df_val_normalized = pd.DataFrame(factor_val_normalized, \n",
    "                                        index=factor_df_val.index, columns=factor_df_val.columns)\n",
    "factor_df_test_normalized = pd.DataFrame(factor_test_normalized, \n",
    "                                         index=factor_df_test.index, columns=factor_df_test.columns)\n",
    "del factor_train_normalized\n",
    "del factor_val_normalized\n",
    "del factor_test_normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给lstm制造时间序列数据\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "seq_length = 10\n",
    "X_train, y_train, index_train = generate_sequence(factor_df_train_normalized, label_y_series_train, seq_length)\n",
    "X_val, y_val, index_val = generate_sequence(factor_df_val_normalized, label_y_series_val, seq_length)\n",
    "X_test, y_test, index_test = generate_sequence(factor_df_test_normalized, label_y_series_test, seq_length)\n",
    "\n",
    "X_train = np.array([factor_seq_df.values for factor_seq_df in X_train]) # 将list 转换为ndarray\n",
    "X_val = np.array([factor_seq_df.values for factor_seq_df in X_val]) # 将list 转换为ndarray\n",
    "X_test = np.array([factor_seq_df.values for factor_seq_df in X_test]) # 将list 转换为ndarray\n",
    "\n",
    "y_cat_train = keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES) # 标签转换为one hot\n",
    "y_cat_val = keras.utils.to_categorical(y_val, num_classes=NUM_CLASSES) # 标签转换为one hot\n",
    "y_cat_test = keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES) # 标签转换为one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu_option():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    keras.backend.tensorflow_backend.set_session(sess)\n",
    "    \n",
    "    return sess\n",
    "    \n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 1\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(CuDNNLSTM(16*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model_dropout(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 2\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    #logging.warning('y_pred: '.format(y_pred))\n",
    "    \"\"\"精确率\"\"\"\n",
    "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
    "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "    precision = tp/ (pp+ K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def Recall(y_true, y_pred):\n",
    "    \"\"\"召回率\"\"\"\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
    "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
    "    recall = tp / (pp + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def F1(y_true, y_pred):\n",
    "    \"\"\"F1-score\"\"\"\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (10, 176)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 10, 128)           156672    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 64)                49664     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 208,581\n",
      "Trainable params: 208,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sess = set_gpu_option()\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print('input_shape: ', input_shape)\n",
    "model = create_lstm_model_dropout(input_shape=input_shape ,num_classes=NUM_CLASSES)\n",
    "adam = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy', keras.metrics.Precision(), keras.metrics.Recall(), F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_grads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_label_train: \n",
      " [0.21709464 0.16683045 0.2442199  0.21006767 0.16178733]\n",
      "majority_label_val: \n",
      " [0.15989976 0.13099416 0.2877193  0.18730159 0.23408522]\n",
      "majority_label_test: \n",
      " [0.28852832 0.21841614 0.15291132 0.25373933 0.08640491]\n"
     ]
    }
   ],
   "source": [
    "majority_label_train = np.sum(y_cat_train, axis=0)/np.sum(y_cat_train)\n",
    "print('majority_label_train: \\n', majority_label_train)\n",
    "majority_label_val = np.sum(y_cat_val, axis=0)/np.sum(y_cat_val)\n",
    "print('majority_label_val: \\n', majority_label_val)\n",
    "majority_label_test = np.sum(y_cat_test, axis=0)/np.sum(y_cat_test)\n",
    "print('majority_label_test: \\n', majority_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53935 samples, validate on 5985 samples\n",
      "Epoch 1/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 1.3290 - categorical_accuracy: 0.4168 - precision_2: 0.6919 - recall_2: 0.1542 - F1: 0.2292 - val_loss: 1.9382 - val_categorical_accuracy: 0.2321 - val_precision_2: 0.2692 - val_recall_2: 0.0428 - val_F1: 0.0672\n",
      "Epoch 2/50\n",
      "53935/53935 [==============================] - 15s 284us/step - loss: 1.2856 - categorical_accuracy: 0.4370 - precision_2: 0.6967 - recall_2: 0.1807 - F1: 0.2623 - val_loss: 2.0162 - val_categorical_accuracy: 0.2294 - val_precision_2: 0.2521 - val_recall_2: 0.0448 - val_F1: 0.0698\n",
      "Epoch 3/50\n",
      "53935/53935 [==============================] - 17s 312us/step - loss: 1.2488 - categorical_accuracy: 0.4560 - precision_2: 0.7026 - recall_2: 0.2054 - F1: 0.2938 - val_loss: 2.1209 - val_categorical_accuracy: 0.2501 - val_precision_2: 0.2855 - val_recall_2: 0.0662 - val_F1: 0.0982\n",
      "Epoch 4/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 1.2031 - categorical_accuracy: 0.4743 - precision_2: 0.7102 - recall_2: 0.2330 - F1: 0.3278 - val_loss: 2.3269 - val_categorical_accuracy: 0.2264 - val_precision_2: 0.2517 - val_recall_2: 0.0687 - val_F1: 0.1011\n",
      "Epoch 5/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 1.1729 - categorical_accuracy: 0.4909 - precision_2: 0.7082 - recall_2: 0.2551 - F1: 0.3521 - val_loss: 2.4448 - val_categorical_accuracy: 0.2246 - val_precision_2: 0.2326 - val_recall_2: 0.0705 - val_F1: 0.1022\n",
      "Epoch 6/50\n",
      "53935/53935 [==============================] - 16s 296us/step - loss: 1.1414 - categorical_accuracy: 0.5047 - precision_2: 0.7136 - recall_2: 0.2805 - F1: 0.3805 - val_loss: 2.4621 - val_categorical_accuracy: 0.2292 - val_precision_2: 0.2425 - val_recall_2: 0.0725 - val_F1: 0.1048\n",
      "Epoch 7/50\n",
      "53935/53935 [==============================] - 16s 296us/step - loss: 1.1049 - categorical_accuracy: 0.5228 - precision_2: 0.7164 - recall_2: 0.3056 - F1: 0.4077 - val_loss: 2.7743 - val_categorical_accuracy: 0.2145 - val_precision_2: 0.2248 - val_recall_2: 0.0787 - val_F1: 0.1109\n",
      "Epoch 8/50\n",
      "53935/53935 [==============================] - 16s 296us/step - loss: 1.0739 - categorical_accuracy: 0.5357 - precision_2: 0.7157 - recall_2: 0.3259 - F1: 0.4278 - val_loss: 2.8882 - val_categorical_accuracy: 0.2102 - val_precision_2: 0.2147 - val_recall_2: 0.0835 - val_F1: 0.1158\n",
      "Epoch 9/50\n",
      "53935/53935 [==============================] - 16s 296us/step - loss: 1.0475 - categorical_accuracy: 0.5469 - precision_2: 0.7173 - recall_2: 0.3470 - F1: 0.4484 - val_loss: 3.1157 - val_categorical_accuracy: 0.2199 - val_precision_2: 0.2372 - val_recall_2: 0.1049 - val_F1: 0.1402\n",
      "Epoch 10/50\n",
      "53935/53935 [==============================] - 16s 294us/step - loss: 1.0282 - categorical_accuracy: 0.5558 - precision_2: 0.7201 - recall_2: 0.3625 - F1: 0.4637 - val_loss: 3.0202 - val_categorical_accuracy: 0.2130 - val_precision_2: 0.2273 - val_recall_2: 0.0939 - val_F1: 0.1274\n",
      "Epoch 11/50\n",
      "53935/53935 [==============================] - 16s 298us/step - loss: 0.9963 - categorical_accuracy: 0.5686 - precision_2: 0.7228 - recall_2: 0.3860 - F1: 0.4861 - val_loss: 3.1648 - val_categorical_accuracy: 0.2152 - val_precision_2: 0.2284 - val_recall_2: 0.0936 - val_F1: 0.1276\n",
      "Epoch 12/50\n",
      "53935/53935 [==============================] - 16s 296us/step - loss: 0.9675 - categorical_accuracy: 0.5833 - precision_2: 0.7260 - recall_2: 0.4081 - F1: 0.5061 - val_loss: 3.2906 - val_categorical_accuracy: 0.2012 - val_precision_2: 0.2198 - val_recall_2: 0.0927 - val_F1: 0.1263\n",
      "Epoch 13/50\n",
      "53935/53935 [==============================] - 16s 294us/step - loss: 0.9500 - categorical_accuracy: 0.5914 - precision_2: 0.7274 - recall_2: 0.4201 - F1: 0.5177 - val_loss: 3.4107 - val_categorical_accuracy: 0.2092 - val_precision_2: 0.2243 - val_recall_2: 0.1028 - val_F1: 0.1367\n",
      "Epoch 14/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 0.9300 - categorical_accuracy: 0.5999 - precision_2: 0.7295 - recall_2: 0.4363 - F1: 0.5318 - val_loss: 3.6433 - val_categorical_accuracy: 0.2137 - val_precision_2: 0.2275 - val_recall_2: 0.1083 - val_F1: 0.1420\n",
      "Epoch 15/50\n",
      "53935/53935 [==============================] - 16s 298us/step - loss: 0.9139 - categorical_accuracy: 0.6080 - precision_2: 0.7314 - recall_2: 0.4517 - F1: 0.5444 - val_loss: 3.5901 - val_categorical_accuracy: 0.2182 - val_precision_2: 0.2321 - val_recall_2: 0.1113 - val_F1: 0.1463\n",
      "Epoch 16/50\n",
      "53935/53935 [==============================] - 16s 295us/step - loss: 0.8984 - categorical_accuracy: 0.6158 - precision_2: 0.7348 - recall_2: 0.4640 - F1: 0.5558 - val_loss: 3.6078 - val_categorical_accuracy: 0.2053 - val_precision_2: 0.2234 - val_recall_2: 0.1073 - val_F1: 0.1403\n",
      "Epoch 17/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 0.8817 - categorical_accuracy: 0.6231 - precision_2: 0.7356 - recall_2: 0.4779 - F1: 0.5669 - val_loss: 3.6913 - val_categorical_accuracy: 0.2038 - val_precision_2: 0.2192 - val_recall_2: 0.1098 - val_F1: 0.1411\n",
      "Epoch 18/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 0.8635 - categorical_accuracy: 0.6317 - precision_2: 0.7365 - recall_2: 0.4904 - F1: 0.5775 - val_loss: 3.7890 - val_categorical_accuracy: 0.2110 - val_precision_2: 0.2294 - val_recall_2: 0.1230 - val_F1: 0.1561\n",
      "Epoch 19/50\n",
      "53935/53935 [==============================] - 16s 297us/step - loss: 0.8506 - categorical_accuracy: 0.6392 - precision_2: 0.7396 - recall_2: 0.5039 - F1: 0.5887 - val_loss: 3.8946 - val_categorical_accuracy: 0.2110 - val_precision_2: 0.2301 - val_recall_2: 0.1282 - val_F1: 0.1616\n",
      "Epoch 20/50\n",
      "53935/53935 [==============================] - 16s 295us/step - loss: 0.8326 - categorical_accuracy: 0.6420 - precision_2: 0.7415 - recall_2: 0.5159 - F1: 0.5980 - val_loss: 3.9976 - val_categorical_accuracy: 0.2069 - val_precision_2: 0.2215 - val_recall_2: 0.1228 - val_F1: 0.1538\n",
      "Epoch 21/50\n",
      "53935/53935 [==============================] - 20s 375us/step - loss: 0.8225 - categorical_accuracy: 0.6514 - precision_2: 0.7445 - recall_2: 0.5258 - F1: 0.6061 - val_loss: 4.0421 - val_categorical_accuracy: 0.2132 - val_precision_2: 0.2303 - val_recall_2: 0.1290 - val_F1: 0.1618\n",
      "Epoch 22/50\n",
      "53935/53935 [==============================] - 20s 364us/step - loss: 0.8066 - categorical_accuracy: 0.6556 - precision_2: 0.7466 - recall_2: 0.5375 - F1: 0.6154 - val_loss: 4.0284 - val_categorical_accuracy: 0.2152 - val_precision_2: 0.2249 - val_recall_2: 0.1308 - val_F1: 0.1615\n",
      "Epoch 23/50\n",
      "53935/53935 [==============================] - 20s 376us/step - loss: 0.7955 - categorical_accuracy: 0.6611 - precision_2: 0.7499 - recall_2: 0.5465 - F1: 0.6236 - val_loss: 3.9386 - val_categorical_accuracy: 0.2122 - val_precision_2: 0.2305 - val_recall_2: 0.1288 - val_F1: 0.1611\n",
      "Epoch 24/50\n",
      "53935/53935 [==============================] - 20s 371us/step - loss: 0.7854 - categorical_accuracy: 0.6649 - precision_2: 0.7529 - recall_2: 0.5515 - F1: 0.6279 - val_loss: 4.2769 - val_categorical_accuracy: 0.2105 - val_precision_2: 0.2308 - val_recall_2: 0.1459 - val_F1: 0.1759\n",
      "Epoch 25/50\n",
      "53935/53935 [==============================] - 20s 369us/step - loss: 0.7741 - categorical_accuracy: 0.6706 - precision_2: 0.7496 - recall_2: 0.5605 - F1: 0.6333 - val_loss: 4.1327 - val_categorical_accuracy: 0.2045 - val_precision_2: 0.2230 - val_recall_2: 0.1362 - val_F1: 0.1663\n",
      "Epoch 26/50\n",
      "53935/53935 [==============================] - 20s 365us/step - loss: 0.7618 - categorical_accuracy: 0.6758 - precision_2: 0.7540 - recall_2: 0.5718 - F1: 0.6425 - val_loss: 4.3394 - val_categorical_accuracy: 0.2047 - val_precision_2: 0.2139 - val_recall_2: 0.1378 - val_F1: 0.1646\n",
      "Epoch 27/50\n",
      "53935/53935 [==============================] - 20s 364us/step - loss: 0.7586 - categorical_accuracy: 0.6758 - precision_2: 0.7519 - recall_2: 0.5750 - F1: 0.6445 - val_loss: 4.4067 - val_categorical_accuracy: 0.2104 - val_precision_2: 0.2263 - val_recall_2: 0.1450 - val_F1: 0.1734\n",
      "Epoch 28/50\n",
      "53935/53935 [==============================] - 20s 369us/step - loss: 0.7422 - categorical_accuracy: 0.6870 - precision_2: 0.7615 - recall_2: 0.5934 - F1: 0.6599 - val_loss: 4.2085 - val_categorical_accuracy: 0.2092 - val_precision_2: 0.2196 - val_recall_2: 0.1375 - val_F1: 0.1644\n",
      "Epoch 29/50\n",
      "53935/53935 [==============================] - 21s 383us/step - loss: 0.7363 - categorical_accuracy: 0.6891 - precision_2: 0.7590 - recall_2: 0.5934 - F1: 0.6592 - val_loss: 4.3292 - val_categorical_accuracy: 0.2134 - val_precision_2: 0.2280 - val_recall_2: 0.1407 - val_F1: 0.1711\n",
      "Epoch 30/50\n",
      "53935/53935 [==============================] - 20s 378us/step - loss: 0.7207 - categorical_accuracy: 0.6963 - precision_2: 0.7638 - recall_2: 0.6044 - F1: 0.6686 - val_loss: 4.3845 - val_categorical_accuracy: 0.2070 - val_precision_2: 0.2173 - val_recall_2: 0.1365 - val_F1: 0.1654\n",
      "Epoch 31/50\n",
      "53935/53935 [==============================] - 21s 384us/step - loss: 0.7248 - categorical_accuracy: 0.6963 - precision_2: 0.7654 - recall_2: 0.6043 - F1: 0.6687 - val_loss: 4.4147 - val_categorical_accuracy: 0.2132 - val_precision_2: 0.2273 - val_recall_2: 0.1490 - val_F1: 0.1773\n",
      "Epoch 32/50\n",
      "53935/53935 [==============================] - 20s 373us/step - loss: 0.7019 - categorical_accuracy: 0.7048 - precision_2: 0.7672 - recall_2: 0.6206 - F1: 0.6803 - val_loss: 4.6172 - val_categorical_accuracy: 0.2115 - val_precision_2: 0.2286 - val_recall_2: 0.1562 - val_F1: 0.1824\n",
      "Epoch 33/50\n",
      "53935/53935 [==============================] - 20s 366us/step - loss: 0.7035 - categorical_accuracy: 0.7049 - precision_2: 0.7677 - recall_2: 0.6208 - F1: 0.6806 - val_loss: 4.5462 - val_categorical_accuracy: 0.2094 - val_precision_2: 0.2233 - val_recall_2: 0.1459 - val_F1: 0.1732\n",
      "Epoch 34/50\n",
      "53935/53935 [==============================] - 20s 377us/step - loss: 0.6934 - categorical_accuracy: 0.7085 - precision_2: 0.7695 - recall_2: 0.6261 - F1: 0.6849 - val_loss: 4.5527 - val_categorical_accuracy: 0.2129 - val_precision_2: 0.2239 - val_recall_2: 0.1586 - val_F1: 0.1824\n",
      "Epoch 35/50\n",
      "53935/53935 [==============================] - 20s 369us/step - loss: 0.6840 - categorical_accuracy: 0.7134 - precision_2: 0.7713 - recall_2: 0.6353 - F1: 0.6915 - val_loss: 4.6636 - val_categorical_accuracy: 0.2134 - val_precision_2: 0.2250 - val_recall_2: 0.1577 - val_F1: 0.1826\n",
      "Epoch 36/50\n",
      "53935/53935 [==============================] - 20s 379us/step - loss: 0.6794 - categorical_accuracy: 0.7130 - precision_2: 0.7752 - recall_2: 0.6368 - F1: 0.6940 - val_loss: 4.8655 - val_categorical_accuracy: 0.2043 - val_precision_2: 0.2198 - val_recall_2: 0.1566 - val_F1: 0.1806\n",
      "Epoch 37/50\n",
      "53935/53935 [==============================] - 20s 367us/step - loss: 0.6718 - categorical_accuracy: 0.7207 - precision_2: 0.7770 - recall_2: 0.6462 - F1: 0.7004 - val_loss: 4.7448 - val_categorical_accuracy: 0.2048 - val_precision_2: 0.2194 - val_recall_2: 0.1515 - val_F1: 0.1763\n",
      "Epoch 38/50\n",
      "53935/53935 [==============================] - 20s 371us/step - loss: 0.6605 - categorical_accuracy: 0.7245 - precision_2: 0.7759 - recall_2: 0.6492 - F1: 0.7021 - val_loss: 4.8650 - val_categorical_accuracy: 0.2062 - val_precision_2: 0.2162 - val_recall_2: 0.1566 - val_F1: 0.1791\n",
      "Epoch 39/50\n",
      "53935/53935 [==============================] - 20s 367us/step - loss: 0.6586 - categorical_accuracy: 0.7254 - precision_2: 0.7795 - recall_2: 0.6555 - F1: 0.7073 - val_loss: 4.8554 - val_categorical_accuracy: 0.2099 - val_precision_2: 0.2191 - val_recall_2: 0.1549 - val_F1: 0.1794\n",
      "Epoch 40/50\n",
      "53935/53935 [==============================] - 20s 377us/step - loss: 0.6524 - categorical_accuracy: 0.7251 - precision_2: 0.7783 - recall_2: 0.6568 - F1: 0.7076 - val_loss: 4.7889 - val_categorical_accuracy: 0.2062 - val_precision_2: 0.2110 - val_recall_2: 0.1525 - val_F1: 0.1747\n",
      "Epoch 41/50\n",
      "53935/53935 [==============================] - 20s 367us/step - loss: 0.6410 - categorical_accuracy: 0.7318 - precision_2: 0.7836 - recall_2: 0.6660 - F1: 0.7154 - val_loss: 4.8274 - val_categorical_accuracy: 0.2027 - val_precision_2: 0.2147 - val_recall_2: 0.1571 - val_F1: 0.1791\n",
      "Epoch 42/50\n",
      "53935/53935 [==============================] - 20s 373us/step - loss: 0.6407 - categorical_accuracy: 0.7305 - precision_2: 0.7840 - recall_2: 0.6631 - F1: 0.7139 - val_loss: 4.9879 - val_categorical_accuracy: 0.2132 - val_precision_2: 0.2176 - val_recall_2: 0.1602 - val_F1: 0.1827\n",
      "Epoch 43/50\n",
      "53935/53935 [==============================] - 20s 362us/step - loss: 0.6305 - categorical_accuracy: 0.7360 - precision_2: 0.7869 - recall_2: 0.6729 - F1: 0.7211 - val_loss: 5.1588 - val_categorical_accuracy: 0.2177 - val_precision_2: 0.2247 - val_recall_2: 0.1641 - val_F1: 0.1865\n",
      "Epoch 44/50\n",
      "53935/53935 [==============================] - 21s 381us/step - loss: 0.6288 - categorical_accuracy: 0.7397 - precision_2: 0.7863 - recall_2: 0.6780 - F1: 0.7241 - val_loss: 5.1304 - val_categorical_accuracy: 0.2130 - val_precision_2: 0.2149 - val_recall_2: 0.1634 - val_F1: 0.1838\n",
      "Epoch 45/50\n",
      "53935/53935 [==============================] - 20s 367us/step - loss: 0.6256 - categorical_accuracy: 0.7411 - precision_2: 0.7888 - recall_2: 0.6779 - F1: 0.7248 - val_loss: 4.9666 - val_categorical_accuracy: 0.2130 - val_precision_2: 0.2134 - val_recall_2: 0.1609 - val_F1: 0.1818\n",
      "Epoch 46/50\n",
      "53935/53935 [==============================] - 21s 384us/step - loss: 0.6147 - categorical_accuracy: 0.7451 - precision_2: 0.7900 - recall_2: 0.6854 - F1: 0.7301 - val_loss: 5.1346 - val_categorical_accuracy: 0.2102 - val_precision_2: 0.2187 - val_recall_2: 0.1629 - val_F1: 0.1857\n",
      "Epoch 47/50\n",
      "53935/53935 [==============================] - 20s 376us/step - loss: 0.6066 - categorical_accuracy: 0.7474 - precision_2: 0.7907 - recall_2: 0.6913 - F1: 0.7340 - val_loss: 5.0181 - val_categorical_accuracy: 0.2170 - val_precision_2: 0.2205 - val_recall_2: 0.1656 - val_F1: 0.1862\n",
      "Epoch 48/50\n",
      "53935/53935 [==============================] - 20s 376us/step - loss: 0.6087 - categorical_accuracy: 0.7488 - precision_2: 0.7941 - recall_2: 0.6925 - F1: 0.7358 - val_loss: 5.1200 - val_categorical_accuracy: 0.2144 - val_precision_2: 0.2214 - val_recall_2: 0.1642 - val_F1: 0.1863\n",
      "Epoch 49/50\n",
      "53935/53935 [==============================] - 20s 362us/step - loss: 0.5979 - categorical_accuracy: 0.7518 - precision_2: 0.7950 - recall_2: 0.6955 - F1: 0.7381 - val_loss: 5.3258 - val_categorical_accuracy: 0.2164 - val_precision_2: 0.2228 - val_recall_2: 0.1713 - val_F1: 0.1912\n",
      "Epoch 50/50\n",
      "53935/53935 [==============================] - 20s 371us/step - loss: 0.5955 - categorical_accuracy: 0.7543 - precision_2: 0.7977 - recall_2: 0.6999 - F1: 0.7419 - val_loss: 5.4332 - val_categorical_accuracy: 0.2147 - val_precision_2: 0.2170 - val_recall_2: 0.1688 - val_F1: 0.1881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f66a3c2a3c8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_cat_train, epochs=50, verbose=True, batch_size=None, validation_data=(X_val, y_cat_val), \n",
    "          shuffle=False)#, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14976/14976 [==============================] - 2s 139us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.919103020276779,\n",
       " 0.22122061252593994,\n",
       " 0.21677398681640625,\n",
       " 0.18656517565250397,\n",
       " 0.1988639384508133]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test, y=y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7456611e-01, 1.1654310e-01, 1.4132613e-01, 2.5481370e-01,\n",
       "        2.1275097e-01],\n",
       "       [3.4269160e-01, 2.7704176e-01, 2.3469804e-01, 1.1959654e-01,\n",
       "        2.5972046e-02],\n",
       "       [3.7739170e-01, 9.2459977e-02, 3.5829518e-02, 3.5073563e-01,\n",
       "        1.4358313e-01],\n",
       "       ...,\n",
       "       [3.4104082e-01, 1.1278135e-01, 8.1267700e-02, 2.9250365e-01,\n",
       "        1.7240644e-01],\n",
       "       [5.6892210e-01, 3.8844913e-01, 1.9255552e-02, 2.3313148e-02,\n",
       "        6.0119033e-05],\n",
       "       [7.6469284e-01, 2.1602356e-01, 4.6928335e-04, 1.8812284e-02,\n",
       "        2.0248576e-06]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.47203258],\n",
       "       [0.        , 0.50471234],\n",
       "       [0.        , 0.49812973],\n",
       "       ...,\n",
       "       [0.        , 0.2736347 ],\n",
       "       [0.        , 0.35306215],\n",
       "       [0.        , 0.7921562 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "y_cat_test * prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.47203258],\n",
       "       [0.        , 0.50471234],\n",
       "       [0.        , 0.49812973],\n",
       "       ...,\n",
       "       [0.        , 0.2736347 ],\n",
       "       [0.        , 0.35306215],\n",
       "       [0.        , 0.7921562 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clip(y_cat_test * prediction, 0, 1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.round(K.clip(y_cat_test * prediction, 0, 1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7693.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.sum(K.round(K.clip(y_cat_test * prediction, 0, 1))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14976.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.sum(K.round(K.clip(prediction, 0, 1))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51368856"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Precision(y_cat_test, prediction).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(prediction, index=index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_df_test = helper_df.loc[prediction_df.index]\n",
    "helper_df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts.charts import Line\n",
    "from pyecharts.globals import CurrentConfig, NotebookType\n",
    "from pyecharts import options as opts\n",
    "\n",
    "CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_LAB\n",
    "\n",
    "line = Line()\n",
    "line.add_xaxis(prediction_df.index.tolist())\n",
    "line.add_yaxis('close', helper_df_test.close)\n",
    "line.add_yaxis('0', prediction_df[0], yaxis_index=1)\n",
    "line.add_yaxis('1', prediction_df[1], yaxis_index=1)\n",
    "line.add_yaxis('2', prediction_df[2], yaxis_index=1)\n",
    "\n",
    "line.set_global_opts(datazoom_opts=opts.DataZoomOpts())\n",
    "line.extend_axis(yaxis=opts.AxisOpts())\n",
    "\n",
    "line.load_javascript()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Index(index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat[:1000].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.join(label_y_series, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.dropna(inplace=True)\n",
    "factor_df[factor_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.where(factor_df == 'inf', other=False).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_store = pd.HDFStore('/home/data/training_x_150.h5', mode='r')\n",
    "variety = 'A'\n",
    "factor_df = factor_store.get(variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.use_inf_as_na = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df[factor_df.isna().any(axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
