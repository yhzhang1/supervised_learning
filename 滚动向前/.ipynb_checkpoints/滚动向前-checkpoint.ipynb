{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, CuDNNLSTM, Dropout, Conv1D, Conv2D, Reshape, Activation, MaxPooling2D, Flatten,\n",
    "                        BatchNormalization)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import huber_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给ret分类\n",
    "def label_ret(ret):\n",
    "    '''\n",
    "    class 0 = [-inf, -0.1]\n",
    "    class 1 = [-0.1, 0.1] unprofitable\n",
    "    class 2 = [0.1, inf]\n",
    "    '''\n",
    "    label = None\n",
    "    if ret < -0.1:\n",
    "        label = 0\n",
    "    elif -0.1 <= ret and ret <= 0.1:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 2\n",
    "        \n",
    "    return label\n",
    "def label_ret_bi(ret):\n",
    "    label = None\n",
    "    if ret <= 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "def label_ret2(ret):\n",
    "\n",
    "    if -0.1 <= ret and ret <= 0.1:\n",
    "        return 0\n",
    "    elif 0.1 < ret and ret <= 0.3:\n",
    "        return 1\n",
    "    elif 0.3 < ret:\n",
    "        return 2\n",
    "    elif -0.3 <= ret and ret < -0.1:\n",
    "        return 3\n",
    "    elif ret < -0.3:\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "        \n",
    "def generate_sequence(X_df, y_series, seq_length):\n",
    "    assert (X_df.index == y_series.index).all()\n",
    "    dataX = list()\n",
    "    dataY = list()\n",
    "    index = list()\n",
    "    for i in range(0, X_df.shape[0] - seq_length + 1):\n",
    "        dataX.append(X_df[i:i+seq_length])\n",
    "        dataY.append(y_series[i+seq_length-1])\n",
    "        index.append(y_series.index[i+seq_length-1])\n",
    "        \n",
    "    return dataX, dataY, pd.Index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety = 'RB'\n",
    "factor_store = pd.HDFStore('/home/data/vb/training_x_150.h5', mode='r')\n",
    "factor_df = factor_store.get(variety)\n",
    "y_store = pd.HDFStore('/home/data/vb/training_y_reg_150.h5', mode='r')\n",
    "y_series = y_store.get(variety)\n",
    "helper_df = pd.read_parquet('/home/data/vb/training_helper_150_{}.parquet'.format(variety))\n",
    "\n",
    "# 对ret做分类\n",
    "ret_y_series = np.exp(y_series) - 1 # 获得回报的原始收益\n",
    "\n",
    "label_y_series = ret_y_series.transform(label_ret_bi).rename('Y_label') # 分类标签\n",
    "ret_label_df = pd.concat([ret_y_series, label_y_series], axis=1) # 合并log ret和label\n",
    "assert (factor_df.index == label_y_series.index).all() # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# 对齐日期 去掉na\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "df = helper_df.join(factor_df, how='inner').join(label_y_series, how='inner')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 再次得到 factor_df, label_y_series, helper_df\n",
    "factor_df = df[factor_df.columns]\n",
    "label_y_series = df[label_y_series.name]\n",
    "helper_df = df[helper_df.columns]\n",
    "\n",
    "assert (factor_df.index == label_y_series.index).all() and \\\n",
    "        (label_y_series.index == helper_df.index).all()     # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# normalize data 在这里会丢失dataframe\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(factor_df)\n",
    "factor_normalized = scaler.transform(factor_df)\n",
    "\n",
    "\n",
    "\n",
    "# 将dataframe的index和columns加回去\n",
    "factor_df_normalized = pd.DataFrame(factor_normalized, \n",
    "                                          index=factor_df.index, columns=factor_df.columns)\n",
    "\n",
    "del factor_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给lstm制造时间序列数据\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "X, y, index = generate_sequence(factor_df_normalized, label_y_series, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "X = np.array([factor_seq_df.values for factor_seq_df in X]) # 将list 转换为ndarray\n",
    "\n",
    "y_cat = keras.utils.to_categorical(y) # 标签转换为one hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu_option():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    keras.backend.tensorflow_backend.set_session(sess)\n",
    "    \n",
    "    return sess\n",
    "    \n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 1\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model_dropout(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 8\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape(input_shape+(1,), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_conv1d_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=20, kernel_size=30, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def Precision(y_true, y_pred):\n",
    "    \"\"\"精确率\"\"\"\n",
    "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
    "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "    precision = tp/ (pp+ K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def Recall(y_true, y_pred):\n",
    "    \"\"\"召回率\"\"\"\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
    "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
    "    recall = tp / (pp + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def F1(y_true, y_pred):\n",
    "    \"\"\"F1-score\"\"\"\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (100, 176)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 71, 20)            105620    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 71, 20)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 42, 30)            18030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 42, 30)            120       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 13, 30)            27030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 13, 30)            120       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 390)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              400384    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 557,530\n",
      "Trainable params: 555,322\n",
      "Non-trainable params: 2,208\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = y_cat.shape[1]\n",
    "sess = set_gpu_option()\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "print('input_shape: ', input_shape)\n",
    "model = create_cnn_conv1d_model(input_shape=input_shape ,num_classes=NUM_CLASSES)\n",
    "adam = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 0s 523us/step - loss: 3.5602e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6260 - val_accuracy: 0.4200 - val_precision_7: 0.4200 - val_recall_7: 0.4200 - val_F1: 0.4420\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 0s 529us/step - loss: 3.4729e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6372 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 523us/step - loss: 3.3882e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6482 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 0s 498us/step - loss: 3.3065e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6593 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 0s 505us/step - loss: 3.2275e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6703 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 0s 487us/step - loss: 3.1512e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6816 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 0s 494us/step - loss: 3.0774e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.6926 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 0s 478us/step - loss: 3.0058e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7033 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 0s 482us/step - loss: 2.9365e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7144 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 0s 503us/step - loss: 2.8692e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7252 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 0s 494us/step - loss: 2.8040e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7357 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 486us/step - loss: 2.7407e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7465 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 0s 485us/step - loss: 2.6792e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7571 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 0s 506us/step - loss: 2.6196e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7676 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 0s 499us/step - loss: 2.5617e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7782 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 0s 501us/step - loss: 2.5055e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7887 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 0s 496us/step - loss: 2.4511e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.7988 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 0s 506us/step - loss: 2.3981e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8093 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 0s 510us/step - loss: 2.3466e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8196 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 0s 509us/step - loss: 2.2967e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8295 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 0s 498us/step - loss: 2.2481e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8394 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 0s 513us/step - loss: 2.2007e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8492 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 0s 511us/step - loss: 2.1547e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8590 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 0s 502us/step - loss: 2.1098e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8685 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 0s 501us/step - loss: 2.0661e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8781 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 0s 514us/step - loss: 2.0235e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8876 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 0s 482us/step - loss: 1.9820e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.8971 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 0s 505us/step - loss: 1.9416e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9064 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 0s 506us/step - loss: 1.9022e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9159 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 0s 509us/step - loss: 1.8638e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9254 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 0s 487us/step - loss: 1.8262e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9347 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 0s 497us/step - loss: 1.7895e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9440 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 0s 484us/step - loss: 1.7538e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9533 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 0s 470us/step - loss: 1.7189e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9627 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 0s 479us/step - loss: 1.6850e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9720 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 0s 512us/step - loss: 1.6519e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9815 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 0s 518us/step - loss: 1.6195e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 4.9906 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 0s 486us/step - loss: 1.5880e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0001 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 0s 502us/step - loss: 1.5572e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0092 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 0s 521us/step - loss: 1.5272e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0182 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 0s 528us/step - loss: 1.4979e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0273 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 0s 523us/step - loss: 1.4693e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0365 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 0s 514us/step - loss: 1.4413e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0453 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 0s 509us/step - loss: 1.4140e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0542 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 0s 494us/step - loss: 1.3874e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0630 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 0s 495us/step - loss: 1.3614e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0717 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 0s 493us/step - loss: 1.3361e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0807 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 0s 475us/step - loss: 1.3113e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0892 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 0s 492us/step - loss: 1.2871e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.0980 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 0s 500us/step - loss: 1.2634e-04 - accuracy: 1.0000 - precision_7: 1.0000 - recall_7: 1.0000 - F1: 1.0000 - val_loss: 5.1068 - val_accuracy: 0.4250 - val_precision_7: 0.4250 - val_recall_7: 0.4250 - val_F1: 0.4464\n"
     ]
    }
   ],
   "source": [
    "num_test = 200\n",
    "num_train = 1000\n",
    "train_start = 0\n",
    "train_end = train_start + num_train\n",
    "val_start = train_end\n",
    "val_end = val_start + num_test\n",
    "\n",
    "while val_end <= X.shape[0]:\n",
    "    model.fit(x=X[train_start:train_end], y=y_cat[train_start:train_end], epochs=10, verbose=True, \n",
    "              batch_size=None, validation_data=(X[val_start:val_end], y_cat[val_start:val_end]), \n",
    "              shuffle=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74824"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rint(model.predict(X[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44, 0.56], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat[-num_test:].sum(axis=0)/y_cat[-num_test:].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
