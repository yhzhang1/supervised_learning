{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, CuDNNLSTM, Dropout, Conv1D, Conv2D, Reshape, Activation, MaxPooling2D, Flatten,\n",
    "                        BatchNormalization)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import huber_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给ret分类\n",
    "def label_ret(ret):\n",
    "    '''\n",
    "    class 0 = [-inf, -0.1]\n",
    "    class 1 = [-0.1, 0.1] unprofitable\n",
    "    class 2 = [0.1, inf]\n",
    "    '''\n",
    "    label = None\n",
    "    if ret < -0.1:\n",
    "        label = 0\n",
    "    elif -0.1 <= ret and ret <= 0.1:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 2\n",
    "        \n",
    "    return label\n",
    "def label_ret_bi(ret):\n",
    "    label = None\n",
    "    if ret <= 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "def label_ret2(ret):\n",
    "\n",
    "    if -0.1 <= ret and ret <= 0.1:\n",
    "        return 0\n",
    "    elif 0.1 < ret and ret <= 0.3:\n",
    "        return 1\n",
    "    elif 0.3 < ret:\n",
    "        return 2\n",
    "    elif -0.3 <= ret and ret < -0.1:\n",
    "        return 3\n",
    "    elif ret < -0.3:\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "        \n",
    "def generate_sequence(X_df, y_series, seq_length):\n",
    "    assert (X_df.index == y_series.index).all()\n",
    "    dataX = list()\n",
    "    dataY = list()\n",
    "    index = list()\n",
    "    for i in range(0, X_df.shape[0] - seq_length + 1):\n",
    "        dataX.append(X_df[i:i+seq_length])\n",
    "        dataY.append(y_series[i+seq_length-1])\n",
    "        index.append(y_series.index[i+seq_length-1])\n",
    "        \n",
    "    return dataX, dataY, pd.Index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety = 'RB'\n",
    "factor_store = pd.HDFStore('/home/data/vb/training_x_150.h5', mode='r')\n",
    "factor_df = factor_store.get(variety)\n",
    "y_store = pd.HDFStore('/home/data/vb/training_y_reg_150.h5', mode='r')\n",
    "y_series = y_store.get(variety)\n",
    "helper_df = pd.read_parquet('/home/data/vb/training_helper_150_{}.parquet'.format(variety))\n",
    "\n",
    "# 对ret做分类\n",
    "ret_y_series = np.exp(y_series) - 1 # 获得回报的原始收益\n",
    "\n",
    "label_y_series = ret_y_series.transform(label_ret_bi).rename('Y_label') # 分类标签\n",
    "ret_label_df = pd.concat([ret_y_series, label_y_series], axis=1) # 合并log ret和label\n",
    "assert (factor_df.index == label_y_series.index).all() # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# 对齐日期 去掉na\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "df = helper_df.join(factor_df, how='inner').join(label_y_series, how='inner')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 再次得到 factor_df, label_y_series, helper_df\n",
    "factor_df = df[factor_df.columns]\n",
    "label_y_series = df[label_y_series.name]\n",
    "helper_df = df[helper_df.columns]\n",
    "\n",
    "assert (factor_df.index == label_y_series.index).all() and \\\n",
    "        (label_y_series.index == helper_df.index).all()     # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# normalize data 在这里会丢失dataframe\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(factor_df)\n",
    "factor_normalized = scaler.transform(factor_df)\n",
    "\n",
    "\n",
    "\n",
    "# 将dataframe的index和columns加回去\n",
    "factor_df_normalized = pd.DataFrame(factor_normalized, \n",
    "                                          index=factor_df.index, columns=factor_df.columns)\n",
    "\n",
    "del factor_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给lstm制造时间序列数据\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "X, y, index = generate_sequence(factor_df_normalized, label_y_series, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "X = np.array([factor_seq_df.values for factor_seq_df in X]) # 将list 转换为ndarray\n",
    "\n",
    "y_cat = keras.utils.to_categorical(y) # 标签转换为one hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu_option():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    keras.backend.tensorflow_backend.set_session(sess)\n",
    "    \n",
    "    return sess\n",
    "    \n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 1\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model_dropout(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 8\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape(input_shape+(1,), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_conv1d_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=20, kernel_size=30, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def Precision(y_true, y_pred):\n",
    "    \"\"\"精确率\"\"\"\n",
    "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
    "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "    precision = tp/ (pp+ K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def Recall(y_true, y_pred):\n",
    "    \"\"\"召回率\"\"\"\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
    "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
    "    recall = tp / (pp + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def F1(y_true, y_pred):\n",
    "    \"\"\"F1-score\"\"\"\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (100, 176)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 71, 20)            105620    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 71, 20)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 42, 30)            18030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 42, 30)            120       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 13, 30)            27030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 30)            120       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 390)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              400384    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 557,530\n",
      "Trainable params: 555,322\n",
      "Non-trainable params: 2,208\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3172: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = y_cat.shape[1]\n",
    "sess = set_gpu_option()\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "print('input_shape: ', input_shape)\n",
    "model = create_cnn_conv1d_model(input_shape=input_shape ,num_classes=NUM_CLASSES)\n",
    "adam = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "704/900 [======================>.......] - ETA: 0s - loss: 1.8610 - accuracy: 0.4844 - precision_1: 0.4844 - recall_1: 0.4844 - F1: 0.4844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1dbdd7d95c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(x=X[-1000:-100], y=y_cat[-1000:-100], epochs=10, verbose=True, \n\u001b[1;32m      2\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               shuffle=False)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=X[-1000:-100], y=y_cat[-1000:-100], epochs=10, verbose=True, \n",
    "              batch_size=None, validation_data=(X[-100:], y_cat[-100:]), \n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 278us/step\n"
     ]
    }
   ],
   "source": [
    "_, accuracy, precision, recall, f1 = model.evaluate(x=X[-100:], y=y_cat[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps:  374.12\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "200/200 [==============================] - 0s 211us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.6071428060531616\n",
      "iter:  1\n",
      "0.27%\n",
      "200/200 [==============================] - 0s 249us/step\n",
      "accuracy:  0.3700000047683716\n",
      "precision:  0.3700000047683716\n",
      "recall:  0.3700000047683716\n",
      "f1:  0.370535671710968\n",
      "iter:  2\n",
      "0.53%\n",
      "200/200 [==============================] - 0s 178us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5089285373687744\n",
      "iter:  3\n",
      "0.80%\n",
      "200/200 [==============================] - 0s 249us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.5223214030265808\n",
      "iter:  4\n",
      "1.07%\n",
      "200/200 [==============================] - 0s 227us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.5133928060531616\n",
      "iter:  5\n",
      "1.34%\n",
      "200/200 [==============================] - 0s 254us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4910714030265808\n",
      "iter:  6\n",
      "1.60%\n",
      "200/200 [==============================] - 0s 181us/step\n",
      "accuracy:  0.6200000047683716\n",
      "precision:  0.6200000047683716\n",
      "recall:  0.6200000047683716\n",
      "f1:  0.5937499403953552\n",
      "iter:  7\n",
      "1.87%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.526785671710968\n",
      "iter:  8\n",
      "2.14%\n",
      "200/200 [==============================] - 0s 214us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.433035671710968\n",
      "iter:  9\n",
      "2.41%\n",
      "200/200 [==============================] - 0s 185us/step\n",
      "accuracy:  0.39500001072883606\n",
      "precision:  0.39500001072883606\n",
      "recall:  0.39500001072883606\n",
      "f1:  0.433035671710968\n",
      "iter:  10\n",
      "2.67%\n",
      "200/200 [==============================] - 0s 192us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.4598214030265808\n",
      "iter:  11\n",
      "2.94%\n",
      "200/200 [==============================] - 0s 168us/step\n",
      "accuracy:  0.3700000047683716\n",
      "precision:  0.3700000047683716\n",
      "recall:  0.3700000047683716\n",
      "f1:  0.4107142388820648\n",
      "iter:  12\n",
      "3.21%\n",
      "200/200 [==============================] - 0s 239us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.4776785373687744\n",
      "iter:  13\n",
      "3.47%\n",
      "200/200 [==============================] - 0s 174us/step\n",
      "accuracy:  0.6449999809265137\n",
      "precision:  0.6449999809265137\n",
      "recall:  0.6449999809265137\n",
      "f1:  0.6562499403953552\n",
      "iter:  14\n",
      "3.74%\n",
      "200/200 [==============================] - 0s 207us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.482142835855484\n",
      "iter:  15\n",
      "4.01%\n",
      "200/200 [==============================] - 0s 164us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5446428060531616\n",
      "iter:  16\n",
      "4.28%\n",
      "200/200 [==============================] - 0s 206us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4598214030265808\n",
      "iter:  17\n",
      "4.54%\n",
      "200/200 [==============================] - 0s 211us/step\n",
      "accuracy:  0.6050000190734863\n",
      "precision:  0.6050000190734863\n",
      "recall:  0.6050000190734863\n",
      "f1:  0.620535671710968\n",
      "iter:  18\n",
      "4.81%\n",
      "200/200 [==============================] - 0s 222us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.4241071045398712\n",
      "iter:  19\n",
      "5.08%\n",
      "200/200 [==============================] - 0s 248us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.589285671710968\n",
      "iter:  20\n",
      "5.35%\n",
      "200/200 [==============================] - 0s 201us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.5535714030265808\n",
      "iter:  21\n",
      "5.61%\n",
      "200/200 [==============================] - 0s 200us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.4910714030265808\n",
      "iter:  22\n",
      "5.88%\n",
      "200/200 [==============================] - 0s 165us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4598214030265808\n",
      "iter:  23\n",
      "6.15%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.4464285373687744\n",
      "iter:  24\n",
      "6.42%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.4099999964237213\n",
      "precision:  0.4099999964237213\n",
      "recall:  0.4099999964237213\n",
      "f1:  0.419642835855484\n",
      "iter:  25\n",
      "6.68%\n",
      "200/200 [==============================] - 0s 310us/step\n",
      "accuracy:  0.6150000095367432\n",
      "precision:  0.6150000095367432\n",
      "recall:  0.6150000095367432\n",
      "f1:  0.616071343421936\n",
      "iter:  26\n",
      "6.95%\n",
      "200/200 [==============================] - 0s 172us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4598214030265808\n",
      "iter:  27\n",
      "7.22%\n",
      "200/200 [==============================] - 0s 276us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.4464285373687744\n",
      "iter:  28\n",
      "7.48%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4062499701976776\n",
      "iter:  29\n",
      "7.75%\n",
      "200/200 [==============================] - 0s 260us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.464285671710968\n",
      "iter:  30\n",
      "8.02%\n",
      "200/200 [==============================] - 0s 213us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.4285714030265808\n",
      "iter:  31\n",
      "8.29%\n",
      "200/200 [==============================] - 0s 283us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.4776785373687744\n",
      "iter:  32\n",
      "8.55%\n",
      "200/200 [==============================] - 0s 273us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4464285373687744\n",
      "iter:  33\n",
      "8.82%\n",
      "200/200 [==============================] - 0s 295us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5803570747375488\n",
      "iter:  34\n",
      "9.09%\n",
      "200/200 [==============================] - 0s 165us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4687499701976776\n",
      "iter:  35\n",
      "9.36%\n",
      "200/200 [==============================] - 0s 246us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.4374999701976776\n",
      "iter:  36\n",
      "9.62%\n",
      "200/200 [==============================] - 0s 172us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.464285671710968\n",
      "iter:  37\n",
      "9.89%\n",
      "200/200 [==============================] - 0s 226us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.495535671710968\n",
      "iter:  38\n",
      "10.16%\n",
      "200/200 [==============================] - 0s 183us/step\n",
      "accuracy:  0.6200000047683716\n",
      "precision:  0.6200000047683716\n",
      "recall:  0.6200000047683716\n",
      "f1:  0.5535714030265808\n",
      "iter:  39\n",
      "10.42%\n",
      "200/200 [==============================] - 0s 167us/step\n",
      "accuracy:  0.6349999904632568\n",
      "precision:  0.6349999904632568\n",
      "recall:  0.6349999904632568\n",
      "f1:  0.6607142090797424\n",
      "iter:  40\n",
      "10.69%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.3700000047683716\n",
      "precision:  0.3700000047683716\n",
      "recall:  0.3700000047683716\n",
      "f1:  0.370535671710968\n",
      "iter:  41\n",
      "10.96%\n",
      "200/200 [==============================] - 0s 165us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.464285671710968\n",
      "iter:  42\n",
      "11.23%\n",
      "200/200 [==============================] - 0s 222us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5401785373687744\n",
      "iter:  43\n",
      "11.49%\n",
      "200/200 [==============================] - 0s 240us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  44\n",
      "11.76%\n",
      "200/200 [==============================] - 0s 166us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.419642835855484\n",
      "iter:  45\n",
      "12.03%\n",
      "200/200 [==============================] - 0s 237us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.5223214030265808\n",
      "iter:  46\n",
      "12.30%\n",
      "200/200 [==============================] - 0s 232us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5401785373687744\n",
      "iter:  47\n",
      "12.56%\n",
      "200/200 [==============================] - 0s 185us/step\n",
      "accuracy:  0.3700000047683716\n",
      "precision:  0.3700000047683716\n",
      "recall:  0.3700000047683716\n",
      "f1:  0.3973214030265808\n",
      "iter:  48\n",
      "12.83%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4866071045398712\n",
      "iter:  49\n",
      "13.10%\n",
      "200/200 [==============================] - 0s 188us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5089285373687744\n",
      "iter:  50\n",
      "13.36%\n",
      "200/200 [==============================] - 0s 293us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5624999403953552\n",
      "iter:  51\n",
      "13.63%\n",
      "200/200 [==============================] - 0s 307us/step\n",
      "accuracy:  0.6299999952316284\n",
      "precision:  0.6299999952316284\n",
      "recall:  0.6299999952316284\n",
      "f1:  0.6696428656578064\n",
      "iter:  52\n",
      "13.90%\n",
      "200/200 [==============================] - 0s 310us/step\n",
      "accuracy:  0.5899999737739563\n",
      "precision:  0.5899999737739563\n",
      "recall:  0.5899999737739563\n",
      "f1:  0.526785671710968\n",
      "iter:  53\n",
      "14.17%\n",
      "200/200 [==============================] - 0s 326us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.4776785373687744\n",
      "iter:  54\n",
      "14.43%\n",
      "200/200 [==============================] - 0s 164us/step\n",
      "accuracy:  0.5950000286102295\n",
      "precision:  0.5950000286102295\n",
      "recall:  0.5950000286102295\n",
      "f1:  0.558035671710968\n",
      "iter:  55\n",
      "14.70%\n",
      "200/200 [==============================] - 0s 270us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4732142388820648\n",
      "iter:  56\n",
      "14.97%\n",
      "200/200 [==============================] - 0s 354us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.464285671710968\n",
      "iter:  57\n",
      "15.24%\n",
      "200/200 [==============================] - 0s 297us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.5982142686843872\n",
      "iter:  58\n",
      "15.50%\n",
      "200/200 [==============================] - 0s 272us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.4776785373687744\n",
      "iter:  59\n",
      "15.77%\n",
      "200/200 [==============================] - 0s 180us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.495535671710968\n",
      "iter:  60\n",
      "16.04%\n",
      "200/200 [==============================] - 0s 290us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.433035671710968\n",
      "iter:  61\n",
      "16.30%\n",
      "200/200 [==============================] - 0s 330us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5178571343421936\n",
      "iter:  62\n",
      "16.57%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.526785671710968\n",
      "iter:  63\n",
      "16.84%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4732142388820648\n",
      "iter:  64\n",
      "17.11%\n",
      "200/200 [==============================] - 0s 167us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4687499701976776\n",
      "iter:  65\n",
      "17.37%\n",
      "200/200 [==============================] - 0s 168us/step\n",
      "accuracy:  0.3149999976158142\n",
      "precision:  0.3149999976158142\n",
      "recall:  0.3149999976158142\n",
      "f1:  0.2812499701976776\n",
      "iter:  66\n",
      "17.64%\n",
      "200/200 [==============================] - 0s 224us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.3973214030265808\n",
      "iter:  67\n",
      "17.91%\n",
      "200/200 [==============================] - 0s 167us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4241071045398712\n",
      "iter:  68\n",
      "18.18%\n",
      "200/200 [==============================] - 0s 197us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5669642686843872\n",
      "iter:  69\n",
      "18.44%\n",
      "200/200 [==============================] - 0s 248us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4999999701976776\n",
      "iter:  70\n",
      "18.71%\n",
      "200/200 [==============================] - 0s 181us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5758928060531616\n",
      "iter:  71\n",
      "18.98%\n",
      "200/200 [==============================] - 0s 225us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.5133928060531616\n",
      "iter:  72\n",
      "19.25%\n",
      "200/200 [==============================] - 0s 179us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4062499701976776\n",
      "iter:  73\n",
      "19.51%\n",
      "200/200 [==============================] - 0s 221us/step\n",
      "accuracy:  0.4099999964237213\n",
      "precision:  0.4099999964237213\n",
      "recall:  0.4099999964237213\n",
      "f1:  0.3794642388820648\n",
      "iter:  74\n",
      "19.78%\n",
      "200/200 [==============================] - 0s 205us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.526785671710968\n",
      "iter:  75\n",
      "20.05%\n",
      "200/200 [==============================] - 0s 224us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  76\n",
      "20.31%\n",
      "200/200 [==============================] - 0s 226us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.5848214030265808\n",
      "iter:  77\n",
      "20.58%\n",
      "200/200 [==============================] - 0s 280us/step\n",
      "accuracy:  0.6150000095367432\n",
      "precision:  0.6150000095367432\n",
      "recall:  0.6150000095367432\n",
      "f1:  0.6428570747375488\n",
      "iter:  78\n",
      "20.85%\n",
      "200/200 [==============================] - 0s 179us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.6294642686843872\n",
      "iter:  79\n",
      "21.12%\n",
      "200/200 [==============================] - 0s 233us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4687499701976776\n",
      "iter:  80\n",
      "21.38%\n",
      "200/200 [==============================] - 0s 266us/step\n",
      "accuracy:  0.6399999856948853\n",
      "precision:  0.6399999856948853\n",
      "recall:  0.6399999856948853\n",
      "f1:  0.6383928060531616\n",
      "iter:  81\n",
      "21.65%\n",
      "200/200 [==============================] - 0s 192us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.4553571045398712\n",
      "iter:  82\n",
      "21.92%\n",
      "200/200 [==============================] - 0s 240us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5312499403953552\n",
      "iter:  83\n",
      "22.19%\n",
      "200/200 [==============================] - 0s 248us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.4464285373687744\n",
      "iter:  84\n",
      "22.45%\n",
      "200/200 [==============================] - 0s 207us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5223214030265808\n",
      "iter:  85\n",
      "22.72%\n",
      "200/200 [==============================] - 0s 169us/step\n",
      "accuracy:  0.44999998807907104\n",
      "precision:  0.44999998807907104\n",
      "recall:  0.44999998807907104\n",
      "f1:  0.4553571045398712\n",
      "iter:  86\n",
      "22.99%\n",
      "200/200 [==============================] - 0s 251us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.4776785373687744\n",
      "iter:  87\n",
      "23.25%\n",
      "200/200 [==============================] - 0s 183us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5803570747375488\n",
      "iter:  88\n",
      "23.52%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.3799999952316284\n",
      "precision:  0.3799999952316284\n",
      "recall:  0.3799999952316284\n",
      "f1:  0.433035671710968\n",
      "iter:  89\n",
      "23.79%\n",
      "200/200 [==============================] - 0s 239us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.5044642686843872\n",
      "iter:  90\n",
      "24.06%\n",
      "200/200 [==============================] - 0s 168us/step\n",
      "accuracy:  0.4300000071525574\n",
      "precision:  0.4300000071525574\n",
      "recall:  0.4300000071525574\n",
      "f1:  0.450892835855484\n",
      "iter:  91\n",
      "24.32%\n",
      "200/200 [==============================] - 0s 217us/step\n",
      "accuracy:  0.6150000095367432\n",
      "precision:  0.6150000095367432\n",
      "recall:  0.6150000095367432\n",
      "f1:  0.616071343421936\n",
      "iter:  92\n",
      "24.59%\n",
      "200/200 [==============================] - 0s 231us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5624999403953552\n",
      "iter:  93\n",
      "24.86%\n",
      "200/200 [==============================] - 0s 216us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5044642686843872\n",
      "iter:  94\n",
      "25.13%\n",
      "200/200 [==============================] - 0s 224us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.401785671710968\n",
      "iter:  95\n",
      "25.39%\n",
      "200/200 [==============================] - 0s 181us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5535714030265808\n",
      "iter:  96\n",
      "25.66%\n",
      "200/200 [==============================] - 0s 219us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4419642388820648\n",
      "iter:  97\n",
      "25.93%\n",
      "200/200 [==============================] - 0s 167us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4285714030265808\n",
      "iter:  98\n",
      "26.19%\n",
      "200/200 [==============================] - 0s 264us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5803571343421936\n",
      "iter:  99\n",
      "26.46%\n",
      "200/200 [==============================] - 0s 263us/step\n",
      "accuracy:  0.4099999964237213\n",
      "precision:  0.4099999964237213\n",
      "recall:  0.4099999964237213\n",
      "f1:  0.4732142388820648\n",
      "iter:  100\n",
      "26.73%\n",
      "200/200 [==============================] - 0s 290us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.4866071045398712\n",
      "iter:  101\n",
      "27.00%\n",
      "200/200 [==============================] - 0s 170us/step\n",
      "accuracy:  0.5950000286102295\n",
      "precision:  0.5950000286102295\n",
      "recall:  0.5950000286102295\n",
      "f1:  0.5446428060531616\n",
      "iter:  102\n",
      "27.26%\n",
      "200/200 [==============================] - 0s 275us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.482142835855484\n",
      "iter:  103\n",
      "27.53%\n",
      "200/200 [==============================] - 0s 268us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.495535671710968\n",
      "iter:  104\n",
      "27.80%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.4464285373687744\n",
      "iter:  105\n",
      "28.07%\n",
      "200/200 [==============================] - 0s 166us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.5357142686843872\n",
      "iter:  106\n",
      "28.33%\n",
      "200/200 [==============================] - 0s 263us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5624999403953552\n",
      "iter:  107\n",
      "28.60%\n",
      "200/200 [==============================] - 0s 263us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.464285671710968\n",
      "iter:  108\n",
      "28.87%\n",
      "200/200 [==============================] - 0s 268us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.4866071045398712\n",
      "iter:  109\n",
      "29.14%\n",
      "200/200 [==============================] - 0s 184us/step\n",
      "accuracy:  0.42500001192092896\n",
      "precision:  0.42500001192092896\n",
      "recall:  0.42500001192092896\n",
      "f1:  0.4062499701976776\n",
      "iter:  110\n",
      "29.40%\n",
      "200/200 [==============================] - 0s 218us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5669642686843872\n",
      "iter:  111\n",
      "29.67%\n",
      "200/200 [==============================] - 0s 173us/step\n",
      "accuracy:  0.5649999976158142\n",
      "precision:  0.5649999976158142\n",
      "recall:  0.5649999976158142\n",
      "f1:  0.5714285373687744\n",
      "iter:  112\n",
      "29.94%\n",
      "200/200 [==============================] - 0s 257us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.401785671710968\n",
      "iter:  113\n",
      "30.20%\n",
      "200/200 [==============================] - 0s 179us/step\n",
      "accuracy:  0.6399999856948853\n",
      "precision:  0.6399999856948853\n",
      "recall:  0.6399999856948853\n",
      "f1:  0.6785714030265808\n",
      "iter:  114\n",
      "30.47%\n",
      "200/200 [==============================] - 0s 165us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.5089285373687744\n",
      "iter:  115\n",
      "30.74%\n",
      "200/200 [==============================] - 0s 297us/step\n",
      "accuracy:  0.5799999833106995\n",
      "precision:  0.5799999833106995\n",
      "recall:  0.5799999833106995\n",
      "f1:  0.5982142090797424\n",
      "iter:  116\n",
      "31.01%\n",
      "200/200 [==============================] - 0s 168us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5535714030265808\n",
      "iter:  117\n",
      "31.27%\n",
      "200/200 [==============================] - 0s 212us/step\n",
      "accuracy:  0.6050000190734863\n",
      "precision:  0.6050000190734863\n",
      "recall:  0.6050000190734863\n",
      "f1:  0.620535671710968\n",
      "iter:  118\n",
      "31.54%\n",
      "200/200 [==============================] - 0s 199us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5044642686843872\n",
      "iter:  119\n",
      "31.81%\n",
      "200/200 [==============================] - 0s 173us/step\n",
      "accuracy:  0.4000000059604645\n",
      "precision:  0.4000000059604645\n",
      "recall:  0.4000000059604645\n",
      "f1:  0.4374999701976776\n",
      "iter:  120\n",
      "32.08%\n",
      "200/200 [==============================] - 0s 280us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.5223214030265808\n",
      "iter:  121\n",
      "32.34%\n",
      "200/200 [==============================] - 0s 180us/step\n",
      "accuracy:  0.41499999165534973\n",
      "precision:  0.41499999165534973\n",
      "recall:  0.41499999165534973\n",
      "f1:  0.4107142388820648\n",
      "iter:  122\n",
      "32.61%\n",
      "200/200 [==============================] - 0s 245us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5758928060531616\n",
      "iter:  123\n",
      "32.88%\n",
      "200/200 [==============================] - 0s 267us/step\n",
      "accuracy:  0.4000000059604645\n",
      "precision:  0.4000000059604645\n",
      "recall:  0.4000000059604645\n",
      "f1:  0.4374999701976776\n",
      "iter:  124\n",
      "33.14%\n",
      "200/200 [==============================] - 0s 173us/step\n",
      "accuracy:  0.42500001192092896\n",
      "precision:  0.42500001192092896\n",
      "recall:  0.42500001192092896\n",
      "f1:  0.3794642388820648\n",
      "iter:  125\n",
      "33.41%\n",
      "200/200 [==============================] - 0s 302us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5446428060531616\n",
      "iter:  126\n",
      "33.68%\n",
      "200/200 [==============================] - 0s 303us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.5758928060531616\n",
      "iter:  127\n",
      "33.95%\n",
      "200/200 [==============================] - 0s 312us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.4910714030265808\n",
      "iter:  128\n",
      "34.21%\n",
      "200/200 [==============================] - 0s 268us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.388392835855484\n",
      "iter:  129\n",
      "34.48%\n",
      "200/200 [==============================] - 0s 197us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5089285373687744\n",
      "iter:  130\n",
      "34.75%\n",
      "200/200 [==============================] - 0s 300us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.558035671710968\n",
      "iter:  131\n",
      "35.02%\n",
      "200/200 [==============================] - 0s 287us/step\n",
      "accuracy:  0.6000000238418579\n",
      "precision:  0.6000000238418579\n",
      "recall:  0.6000000238418579\n",
      "f1:  0.6428571343421936\n",
      "iter:  132\n",
      "35.28%\n",
      "200/200 [==============================] - 0s 294us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.5133928060531616\n",
      "iter:  133\n",
      "35.55%\n",
      "200/200 [==============================] - 0s 250us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.388392835855484\n",
      "iter:  134\n",
      "35.82%\n",
      "200/200 [==============================] - 0s 262us/step\n",
      "accuracy:  0.4099999964237213\n",
      "precision:  0.4099999964237213\n",
      "recall:  0.4099999964237213\n",
      "f1:  0.4598214030265808\n",
      "iter:  135\n",
      "36.08%\n",
      "200/200 [==============================] - 0s 312us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4598214030265808\n",
      "iter:  136\n",
      "36.35%\n",
      "200/200 [==============================] - 0s 280us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.5446428060531616\n",
      "iter:  137\n",
      "36.62%\n",
      "200/200 [==============================] - 0s 310us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.4151785373687744\n",
      "iter:  138\n",
      "36.89%\n",
      "200/200 [==============================] - 0s 163us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5089285373687744\n",
      "iter:  139\n",
      "37.15%\n",
      "200/200 [==============================] - 0s 224us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5133928060531616\n",
      "iter:  140\n",
      "37.42%\n",
      "200/200 [==============================] - 0s 187us/step\n",
      "accuracy:  0.4050000011920929\n",
      "precision:  0.4050000011920929\n",
      "recall:  0.4050000011920929\n",
      "f1:  0.401785671710968\n",
      "iter:  141\n",
      "37.69%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.3400000035762787\n",
      "precision:  0.3400000035762787\n",
      "recall:  0.3400000035762787\n",
      "f1:  0.357142835855484\n",
      "iter:  142\n",
      "37.96%\n",
      "200/200 [==============================] - 0s 204us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4241071045398712\n",
      "iter:  143\n",
      "38.22%\n",
      "200/200 [==============================] - 0s 252us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.5133928060531616\n",
      "iter:  144\n",
      "38.49%\n",
      "200/200 [==============================] - 0s 179us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.6026785969734192\n",
      "iter:  145\n",
      "38.76%\n",
      "200/200 [==============================] - 0s 178us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5491071343421936\n",
      "iter:  146\n",
      "39.02%\n",
      "200/200 [==============================] - 0s 234us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4464285373687744\n",
      "iter:  147\n",
      "39.29%\n",
      "200/200 [==============================] - 0s 254us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.4107142388820648\n",
      "iter:  148\n",
      "39.56%\n",
      "200/200 [==============================] - 0s 229us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4598214030265808\n",
      "iter:  149\n",
      "39.83%\n",
      "200/200 [==============================] - 0s 309us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.5223214030265808\n",
      "iter:  150\n",
      "40.09%\n",
      "200/200 [==============================] - 0s 217us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.4999999701976776\n",
      "iter:  151\n",
      "40.36%\n",
      "200/200 [==============================] - 0s 250us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5089285373687744\n",
      "iter:  152\n",
      "40.63%\n",
      "200/200 [==============================] - 0s 337us/step\n",
      "accuracy:  0.6200000047683716\n",
      "precision:  0.6200000047683716\n",
      "recall:  0.6200000047683716\n",
      "f1:  0.6607142686843872\n",
      "iter:  153\n",
      "40.90%\n",
      "200/200 [==============================] - 0s 183us/step\n",
      "accuracy:  0.5799999833106995\n",
      "precision:  0.5799999833106995\n",
      "recall:  0.5799999833106995\n",
      "f1:  0.5446428060531616\n",
      "iter:  154\n",
      "41.16%\n",
      "200/200 [==============================] - 0s 240us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5357142686843872\n",
      "iter:  155\n",
      "41.43%\n",
      "200/200 [==============================] - 0s 221us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.5357142686843872\n",
      "iter:  156\n",
      "41.70%\n",
      "200/200 [==============================] - 0s 232us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5714285373687744\n",
      "iter:  157\n",
      "41.97%\n",
      "200/200 [==============================] - 0s 301us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.4776785373687744\n",
      "iter:  158\n",
      "42.23%\n",
      "200/200 [==============================] - 0s 235us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.5982142686843872\n",
      "iter:  159\n",
      "42.50%\n",
      "200/200 [==============================] - 0s 354us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.4910714030265808\n",
      "iter:  160\n",
      "42.77%\n",
      "200/200 [==============================] - 0s 239us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.482142835855484\n",
      "iter:  161\n",
      "43.03%\n",
      "200/200 [==============================] - 0s 306us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.4999999701976776\n",
      "iter:  162\n",
      "43.30%\n",
      "200/200 [==============================] - 0s 443us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.495535671710968\n",
      "iter:  163\n",
      "43.57%\n",
      "200/200 [==============================] - 0s 302us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.4776785373687744\n",
      "iter:  164\n",
      "43.84%\n",
      "200/200 [==============================] - 0s 247us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.5223214030265808\n",
      "iter:  165\n",
      "44.10%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.5133928060531616\n",
      "iter:  166\n",
      "44.37%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4553571045398712\n",
      "iter:  167\n",
      "44.64%\n",
      "200/200 [==============================] - 0s 154us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5223214030265808\n",
      "iter:  168\n",
      "44.91%\n",
      "200/200 [==============================] - 0s 574us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5223214030265808\n",
      "iter:  169\n",
      "45.17%\n",
      "200/200 [==============================] - 0s 719us/step\n",
      "accuracy:  0.5799999833106995\n",
      "precision:  0.5799999833106995\n",
      "recall:  0.5799999833106995\n",
      "f1:  0.625\n",
      "iter:  170\n",
      "45.44%\n",
      "200/200 [==============================] - 0s 190us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.433035671710968\n",
      "iter:  171\n",
      "45.71%\n",
      "200/200 [==============================] - 0s 254us/step\n",
      "accuracy:  0.6549999713897705\n",
      "precision:  0.6549999713897705\n",
      "recall:  0.6549999713897705\n",
      "f1:  0.6651784777641296\n",
      "iter:  172\n",
      "45.97%\n",
      "200/200 [==============================] - 0s 228us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.4419642388820648\n",
      "iter:  173\n",
      "46.24%\n",
      "200/200 [==============================] - 0s 670us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4062499701976776\n",
      "iter:  174\n",
      "46.51%\n",
      "200/200 [==============================] - 0s 364us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4866071045398712\n",
      "iter:  175\n",
      "46.78%\n",
      "200/200 [==============================] - 0s 449us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.450892835855484\n",
      "iter:  176\n",
      "47.04%\n",
      "200/200 [==============================] - 0s 300us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.482142835855484\n",
      "iter:  177\n",
      "47.31%\n",
      "200/200 [==============================] - 0s 203us/step\n",
      "accuracy:  0.6000000238418579\n",
      "precision:  0.6000000238418579\n",
      "recall:  0.6000000238418579\n",
      "f1:  0.5491071343421936\n",
      "iter:  178\n",
      "47.58%\n",
      "200/200 [==============================] - 0s 240us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5446428060531616\n",
      "iter:  179\n",
      "47.85%\n",
      "200/200 [==============================] - 0s 170us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.4910714030265808\n",
      "iter:  180\n",
      "48.11%\n",
      "200/200 [==============================] - 0s 241us/step\n",
      "accuracy:  0.3799999952316284\n",
      "precision:  0.3799999952316284\n",
      "recall:  0.3799999952316284\n",
      "f1:  0.3526785373687744\n",
      "iter:  181\n",
      "48.38%\n",
      "200/200 [==============================] - 0s 163us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.419642835855484\n",
      "iter:  182\n",
      "48.65%\n",
      "200/200 [==============================] - 0s 264us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.4285714030265808\n",
      "iter:  183\n",
      "48.91%\n",
      "200/200 [==============================] - 0s 222us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.5312499403953552\n",
      "iter:  184\n",
      "49.18%\n",
      "200/200 [==============================] - 0s 249us/step\n",
      "accuracy:  0.35499998927116394\n",
      "precision:  0.35499998927116394\n",
      "recall:  0.35499998927116394\n",
      "f1:  0.370535671710968\n",
      "iter:  185\n",
      "49.45%\n",
      "200/200 [==============================] - 0s 281us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.5714285373687744\n",
      "iter:  186\n",
      "49.72%\n",
      "200/200 [==============================] - 0s 321us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.5535714030265808\n",
      "iter:  187\n",
      "49.98%\n",
      "200/200 [==============================] - 0s 245us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.526785671710968\n",
      "iter:  188\n",
      "50.25%\n",
      "200/200 [==============================] - 0s 227us/step\n",
      "accuracy:  0.6000000238418579\n",
      "precision:  0.6000000238418579\n",
      "recall:  0.6000000238418579\n",
      "f1:  0.6026784777641296\n",
      "iter:  189\n",
      "50.52%\n",
      "200/200 [==============================] - 0s 219us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.4687499701976776\n",
      "iter:  190\n",
      "50.79%\n",
      "200/200 [==============================] - 0s 409us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.464285671710968\n",
      "iter:  191\n",
      "51.05%\n",
      "200/200 [==============================] - 0s 242us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4732142388820648\n",
      "iter:  192\n",
      "51.32%\n",
      "200/200 [==============================] - 0s 288us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  193\n",
      "51.59%\n",
      "200/200 [==============================] - 0s 216us/step\n",
      "accuracy:  0.44999998807907104\n",
      "precision:  0.44999998807907104\n",
      "recall:  0.44999998807907104\n",
      "f1:  0.4285714030265808\n",
      "iter:  194\n",
      "51.86%\n",
      "200/200 [==============================] - 0s 222us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.6071428060531616\n",
      "iter:  195\n",
      "52.12%\n",
      "200/200 [==============================] - 0s 201us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  196\n",
      "52.39%\n",
      "200/200 [==============================] - 0s 287us/step\n",
      "accuracy:  0.36500000953674316\n",
      "precision:  0.36500000953674316\n",
      "recall:  0.36500000953674316\n",
      "f1:  0.3928571045398712\n",
      "iter:  197\n",
      "52.66%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.5133928060531616\n",
      "iter:  198\n",
      "52.92%\n",
      "200/200 [==============================] - 0s 461us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.5044642686843872\n",
      "iter:  199\n",
      "53.19%\n",
      "200/200 [==============================] - 0s 257us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4553571045398712\n",
      "iter:  200\n",
      "53.46%\n",
      "200/200 [==============================] - 0s 263us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.5535714030265808\n",
      "iter:  201\n",
      "53.73%\n",
      "200/200 [==============================] - 0s 259us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.6026785969734192\n",
      "iter:  202\n",
      "53.99%\n",
      "200/200 [==============================] - 0s 272us/step\n",
      "accuracy:  0.6449999809265137\n",
      "precision:  0.6449999809265137\n",
      "recall:  0.6449999809265137\n",
      "f1:  0.6428570747375488\n",
      "iter:  203\n",
      "54.26%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  204\n",
      "54.53%\n",
      "200/200 [==============================] - 0s 264us/step\n",
      "accuracy:  0.6050000190734863\n",
      "precision:  0.6050000190734863\n",
      "recall:  0.6050000190734863\n",
      "f1:  0.620535671710968\n",
      "iter:  205\n",
      "54.80%\n",
      "200/200 [==============================] - 0s 231us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4241071045398712\n",
      "iter:  206\n",
      "55.06%\n",
      "200/200 [==============================] - 0s 222us/step\n",
      "accuracy:  0.42500001192092896\n",
      "precision:  0.42500001192092896\n",
      "recall:  0.42500001192092896\n",
      "f1:  0.419642835855484\n",
      "iter:  207\n",
      "55.33%\n",
      "200/200 [==============================] - 0s 236us/step\n",
      "accuracy:  0.4050000011920929\n",
      "precision:  0.4050000011920929\n",
      "recall:  0.4050000011920929\n",
      "f1:  0.4419642388820648\n",
      "iter:  208\n",
      "55.60%\n",
      "200/200 [==============================] - 0s 189us/step\n",
      "accuracy:  0.375\n",
      "precision:  0.375\n",
      "recall:  0.375\n",
      "f1:  0.3482142388820648\n",
      "iter:  209\n",
      "55.86%\n",
      "200/200 [==============================] - 0s 224us/step\n",
      "accuracy:  0.4300000071525574\n",
      "precision:  0.4300000071525574\n",
      "recall:  0.4300000071525574\n",
      "f1:  0.4776785373687744\n",
      "iter:  210\n",
      "56.13%\n",
      "200/200 [==============================] - 0s 314us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.5491071343421936\n",
      "iter:  211\n",
      "56.40%\n",
      "200/200 [==============================] - 0s 723us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.5178571343421936\n",
      "iter:  212\n",
      "56.67%\n",
      "200/200 [==============================] - 0s 225us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5535714030265808\n",
      "iter:  213\n",
      "56.93%\n",
      "200/200 [==============================] - 0s 200us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.526785671710968\n",
      "iter:  214\n",
      "57.20%\n",
      "200/200 [==============================] - 0s 346us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.401785671710968\n",
      "iter:  215\n",
      "57.47%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.4374999701976776\n",
      "iter:  216\n",
      "57.74%\n",
      "200/200 [==============================] - 0s 246us/step\n",
      "accuracy:  0.6399999856948853\n",
      "precision:  0.6399999856948853\n",
      "recall:  0.6399999856948853\n",
      "f1:  0.6651784777641296\n",
      "iter:  217\n",
      "58.00%\n",
      "200/200 [==============================] - 0s 186us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.4910714030265808\n",
      "iter:  218\n",
      "58.27%\n",
      "200/200 [==============================] - 0s 200us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5535714030265808\n",
      "iter:  219\n",
      "58.54%\n",
      "200/200 [==============================] - 0s 180us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5312499403953552\n",
      "iter:  220\n",
      "58.80%\n",
      "200/200 [==============================] - 0s 211us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.526785671710968\n",
      "iter:  221\n",
      "59.07%\n",
      "200/200 [==============================] - 0s 208us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.388392835855484\n",
      "iter:  222\n",
      "59.34%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.48500001430511475\n",
      "precision:  0.48500001430511475\n",
      "recall:  0.48500001430511475\n",
      "f1:  0.4598214030265808\n",
      "iter:  223\n",
      "59.61%\n",
      "200/200 [==============================] - 0s 277us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5624999403953552\n",
      "iter:  224\n",
      "59.87%\n",
      "200/200 [==============================] - 0s 189us/step\n",
      "accuracy:  0.4300000071525574\n",
      "precision:  0.4300000071525574\n",
      "recall:  0.4300000071525574\n",
      "f1:  0.4107142388820648\n",
      "iter:  225\n",
      "60.14%\n",
      "200/200 [==============================] - 0s 186us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.419642835855484\n",
      "iter:  226\n",
      "60.41%\n",
      "200/200 [==============================] - 0s 605us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.589285671710968\n",
      "iter:  227\n",
      "60.68%\n",
      "200/200 [==============================] - 0s 229us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5803570747375488\n",
      "iter:  228\n",
      "60.94%\n",
      "200/200 [==============================] - 0s 271us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5312499403953552\n",
      "iter:  229\n",
      "61.21%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5312499403953552\n",
      "iter:  230\n",
      "61.48%\n",
      "200/200 [==============================] - 0s 283us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.5223214030265808\n",
      "iter:  231\n",
      "61.74%\n",
      "200/200 [==============================] - 0s 212us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4598214030265808\n",
      "iter:  232\n",
      "62.01%\n",
      "200/200 [==============================] - 0s 219us/step\n",
      "accuracy:  0.625\n",
      "precision:  0.625\n",
      "recall:  0.625\n",
      "f1:  0.6116070747375488\n",
      "iter:  233\n",
      "62.28%\n",
      "200/200 [==============================] - 0s 213us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.6160714030265808\n",
      "iter:  234\n",
      "62.55%\n",
      "200/200 [==============================] - 0s 199us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.4151785373687744\n",
      "iter:  235\n",
      "62.81%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5535714030265808\n",
      "iter:  236\n",
      "63.08%\n",
      "200/200 [==============================] - 0s 501us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.5133928060531616\n",
      "iter:  237\n",
      "63.35%\n",
      "200/200 [==============================] - 0s 218us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.5133928060531616\n",
      "iter:  238\n",
      "63.62%\n",
      "200/200 [==============================] - 0s 287us/step\n",
      "accuracy:  0.3799999952316284\n",
      "precision:  0.3799999952316284\n",
      "recall:  0.3799999952316284\n",
      "f1:  0.3660714030265808\n",
      "iter:  239\n",
      "63.88%\n",
      "200/200 [==============================] - 0s 228us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.5669642686843872\n",
      "iter:  240\n",
      "64.15%\n",
      "200/200 [==============================] - 0s 803us/step\n",
      "accuracy:  0.4099999964237213\n",
      "precision:  0.4099999964237213\n",
      "recall:  0.4099999964237213\n",
      "f1:  0.3660714030265808\n",
      "iter:  241\n",
      "64.42%\n",
      "200/200 [==============================] - 0s 309us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.5133928060531616\n",
      "iter:  242\n",
      "64.69%\n",
      "200/200 [==============================] - 0s 194us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.5133928060531616\n",
      "iter:  243\n",
      "64.95%\n",
      "200/200 [==============================] - 0s 229us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.464285671710968\n",
      "iter:  244\n",
      "65.22%\n",
      "200/200 [==============================] - 0s 441us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.526785671710968\n",
      "iter:  245\n",
      "65.49%\n",
      "200/200 [==============================] - 0s 688us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.4776785373687744\n",
      "iter:  246\n",
      "65.75%\n",
      "200/200 [==============================] - 0s 157us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4910714030265808\n",
      "iter:  247\n",
      "66.02%\n",
      "200/200 [==============================] - 0s 276us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.5044642686843872\n",
      "iter:  248\n",
      "66.29%\n",
      "200/200 [==============================] - 0s 199us/step\n",
      "accuracy:  0.6000000238418579\n",
      "precision:  0.6000000238418579\n",
      "recall:  0.6000000238418579\n",
      "f1:  0.5491071343421936\n",
      "iter:  249\n",
      "66.56%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.5491071343421936\n",
      "iter:  250\n",
      "66.82%\n",
      "200/200 [==============================] - 0s 233us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4151785373687744\n",
      "iter:  251\n",
      "67.09%\n",
      "200/200 [==============================] - 0s 260us/step\n",
      "accuracy:  0.3199999928474426\n",
      "precision:  0.3199999928474426\n",
      "recall:  0.3199999928474426\n",
      "f1:  0.2991071045398712\n",
      "iter:  252\n",
      "67.36%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.5178571343421936\n",
      "iter:  253\n",
      "67.63%\n",
      "200/200 [==============================] - 0s 180us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4866071045398712\n",
      "iter:  254\n",
      "67.89%\n",
      "200/200 [==============================] - 0s 527us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5937499403953552\n",
      "iter:  255\n",
      "68.16%\n",
      "200/200 [==============================] - 0s 273us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.5669642686843872\n",
      "iter:  256\n",
      "68.43%\n",
      "200/200 [==============================] - 0s 289us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.3482142388820648\n",
      "iter:  257\n",
      "68.69%\n",
      "200/200 [==============================] - 0s 292us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4374999701976776\n",
      "iter:  258\n",
      "68.96%\n",
      "200/200 [==============================] - 0s 307us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5178571343421936\n",
      "iter:  259\n",
      "69.23%\n",
      "200/200 [==============================] - 0s 203us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.5491071343421936\n",
      "iter:  260\n",
      "69.50%\n",
      "200/200 [==============================] - 0s 178us/step\n",
      "accuracy:  0.42500001192092896\n",
      "precision:  0.42500001192092896\n",
      "recall:  0.42500001192092896\n",
      "f1:  0.3928571045398712\n",
      "iter:  261\n",
      "69.76%\n",
      "200/200 [==============================] - 0s 245us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.5624999403953552\n",
      "iter:  262\n",
      "70.03%\n",
      "200/200 [==============================] - 0s 211us/step\n",
      "accuracy:  0.42500001192092896\n",
      "precision:  0.42500001192092896\n",
      "recall:  0.42500001192092896\n",
      "f1:  0.433035671710968\n",
      "iter:  263\n",
      "70.30%\n",
      "200/200 [==============================] - 0s 410us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5803570747375488\n",
      "iter:  264\n",
      "70.57%\n",
      "200/200 [==============================] - 0s 225us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.5044642686843872\n",
      "iter:  265\n",
      "70.83%\n",
      "200/200 [==============================] - 0s 153us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.495535671710968\n",
      "iter:  266\n",
      "71.10%\n",
      "200/200 [==============================] - 0s 323us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5044642686843872\n",
      "iter:  267\n",
      "71.37%\n",
      "200/200 [==============================] - 0s 217us/step\n",
      "accuracy:  0.6200000047683716\n",
      "precision:  0.6200000047683716\n",
      "recall:  0.6200000047683716\n",
      "f1:  0.6339284777641296\n",
      "iter:  268\n",
      "71.63%\n",
      "200/200 [==============================] - 0s 618us/step\n",
      "accuracy:  0.6150000095367432\n",
      "precision:  0.6150000095367432\n",
      "recall:  0.6150000095367432\n",
      "f1:  0.65625\n",
      "iter:  269\n",
      "71.90%\n",
      "200/200 [==============================] - 0s 213us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.482142835855484\n",
      "iter:  270\n",
      "72.17%\n",
      "200/200 [==============================] - 0s 344us/step\n",
      "accuracy:  0.4749999940395355\n",
      "precision:  0.4749999940395355\n",
      "recall:  0.4749999940395355\n",
      "f1:  0.4374999701976776\n",
      "iter:  271\n",
      "72.44%\n",
      "200/200 [==============================] - 0s 419us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4866071045398712\n",
      "iter:  272\n",
      "72.70%\n",
      "200/200 [==============================] - 0s 220us/step\n",
      "accuracy:  0.38499999046325684\n",
      "precision:  0.38499999046325684\n",
      "recall:  0.38499999046325684\n",
      "f1:  0.3973214030265808\n",
      "iter:  273\n",
      "72.97%\n",
      "200/200 [==============================] - 0s 226us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4553571045398712\n",
      "iter:  274\n",
      "73.24%\n",
      "200/200 [==============================] - 0s 381us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.4910714030265808\n",
      "iter:  275\n",
      "73.51%\n",
      "200/200 [==============================] - 0s 429us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.589285671710968\n",
      "iter:  276\n",
      "73.77%\n",
      "200/200 [==============================] - 0s 399us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.4553571045398712\n",
      "iter:  277\n",
      "74.04%\n",
      "200/200 [==============================] - 0s 285us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.5446428060531616\n",
      "iter:  278\n",
      "74.31%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.495535671710968\n",
      "iter:  279\n",
      "74.58%\n",
      "200/200 [==============================] - 0s 232us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5624999403953552\n",
      "iter:  280\n",
      "74.84%\n",
      "200/200 [==============================] - 0s 228us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.5937499403953552\n",
      "iter:  281\n",
      "75.11%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.464285671710968\n",
      "iter:  282\n",
      "75.38%\n",
      "200/200 [==============================] - 0s 251us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.4285714030265808\n",
      "iter:  283\n",
      "75.64%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.388392835855484\n",
      "iter:  284\n",
      "75.91%\n",
      "200/200 [==============================] - 0s 188us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.464285671710968\n",
      "iter:  285\n",
      "76.18%\n",
      "200/200 [==============================] - 0s 315us/step\n",
      "accuracy:  0.44999998807907104\n",
      "precision:  0.44999998807907104\n",
      "recall:  0.44999998807907104\n",
      "f1:  0.4285714030265808\n",
      "iter:  286\n",
      "76.45%\n",
      "200/200 [==============================] - 0s 275us/step\n",
      "accuracy:  0.625\n",
      "precision:  0.625\n",
      "recall:  0.625\n",
      "f1:  0.6383928060531616\n",
      "iter:  287\n",
      "76.71%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5089285373687744\n",
      "iter:  288\n",
      "76.98%\n",
      "200/200 [==============================] - 0s 242us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.558035671710968\n",
      "iter:  289\n",
      "77.25%\n",
      "200/200 [==============================] - 0s 219us/step\n",
      "accuracy:  0.375\n",
      "precision:  0.375\n",
      "recall:  0.375\n",
      "f1:  0.3482142388820648\n",
      "iter:  290\n",
      "77.52%\n",
      "200/200 [==============================] - 0s 263us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5178571343421936\n",
      "iter:  291\n",
      "77.78%\n",
      "200/200 [==============================] - 0s 262us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.5044642686843872\n",
      "iter:  292\n",
      "78.05%\n",
      "200/200 [==============================] - 0s 273us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5089285373687744\n",
      "iter:  293\n",
      "78.32%\n",
      "200/200 [==============================] - 0s 291us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5223214030265808\n",
      "iter:  294\n",
      "78.58%\n",
      "200/200 [==============================] - 0s 289us/step\n",
      "accuracy:  0.5899999737739563\n",
      "precision:  0.5899999737739563\n",
      "recall:  0.5899999737739563\n",
      "f1:  0.5937499403953552\n",
      "iter:  295\n",
      "78.85%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.4910714030265808\n",
      "iter:  296\n",
      "79.12%\n",
      "200/200 [==============================] - 0s 566us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.4910714030265808\n",
      "iter:  297\n",
      "79.39%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.526785671710968\n",
      "iter:  298\n",
      "79.65%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5803570747375488\n",
      "iter:  299\n",
      "79.92%\n",
      "200/200 [==============================] - 0s 301us/step\n",
      "accuracy:  0.4300000071525574\n",
      "precision:  0.4300000071525574\n",
      "recall:  0.4300000071525574\n",
      "f1:  0.450892835855484\n",
      "iter:  300\n",
      "80.19%\n",
      "200/200 [==============================] - 0s 359us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5312499403953552\n",
      "iter:  301\n",
      "80.46%\n",
      "200/200 [==============================] - 0s 207us/step\n",
      "accuracy:  0.46000000834465027\n",
      "precision:  0.46000000834465027\n",
      "recall:  0.46000000834465027\n",
      "f1:  0.450892835855484\n",
      "iter:  302\n",
      "80.72%\n",
      "200/200 [==============================] - 0s 532us/step\n",
      "accuracy:  0.41499999165534973\n",
      "precision:  0.41499999165534973\n",
      "recall:  0.41499999165534973\n",
      "f1:  0.3839285373687744\n",
      "iter:  303\n",
      "80.99%\n",
      "200/200 [==============================] - 0s 227us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.388392835855484\n",
      "iter:  304\n",
      "81.26%\n",
      "200/200 [==============================] - 0s 300us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5178571343421936\n",
      "iter:  305\n",
      "81.52%\n",
      "200/200 [==============================] - 0s 162us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4999999701976776\n",
      "iter:  306\n",
      "81.79%\n",
      "200/200 [==============================] - 0s 270us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4464285373687744\n",
      "iter:  307\n",
      "82.06%\n",
      "200/200 [==============================] - 0s 203us/step\n",
      "accuracy:  0.41499999165534973\n",
      "precision:  0.41499999165534973\n",
      "recall:  0.41499999165534973\n",
      "f1:  0.4776785373687744\n",
      "iter:  308\n",
      "82.33%\n",
      "200/200 [==============================] - 0s 238us/step\n",
      "accuracy:  0.5450000166893005\n",
      "precision:  0.5450000166893005\n",
      "recall:  0.5450000166893005\n",
      "f1:  0.4999999701976776\n",
      "iter:  309\n",
      "82.59%\n",
      "200/200 [==============================] - 0s 202us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4732142388820648\n",
      "iter:  310\n",
      "82.86%\n",
      "200/200 [==============================] - 0s 244us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.558035671710968\n",
      "iter:  311\n",
      "83.13%\n",
      "200/200 [==============================] - 0s 261us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.4553571045398712\n",
      "iter:  312\n",
      "83.40%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.5089285373687744\n",
      "iter:  313\n",
      "83.66%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.4107142388820648\n",
      "iter:  314\n",
      "83.93%\n",
      "200/200 [==============================] - 0s 216us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4732142388820648\n",
      "iter:  315\n",
      "84.20%\n",
      "200/200 [==============================] - 0s 314us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.401785671710968\n",
      "iter:  316\n",
      "84.46%\n",
      "200/200 [==============================] - 0s 193us/step\n",
      "accuracy:  0.39500001072883606\n",
      "precision:  0.39500001072883606\n",
      "recall:  0.39500001072883606\n",
      "f1:  0.3794642388820648\n",
      "iter:  317\n",
      "84.73%\n",
      "200/200 [==============================] - 0s 251us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.450892835855484\n",
      "iter:  318\n",
      "85.00%\n",
      "200/200 [==============================] - 0s 296us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.5089285373687744\n",
      "iter:  319\n",
      "85.27%\n",
      "200/200 [==============================] - 0s 228us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5044642686843872\n",
      "iter:  320\n",
      "85.53%\n",
      "200/200 [==============================] - 0s 190us/step\n",
      "accuracy:  0.4350000023841858\n",
      "precision:  0.4350000023841858\n",
      "recall:  0.4350000023841858\n",
      "f1:  0.4285714030265808\n",
      "iter:  321\n",
      "85.80%\n",
      "200/200 [==============================] - 0s 258us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.3749999701976776\n",
      "iter:  322\n",
      "86.07%\n",
      "200/200 [==============================] - 0s 264us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4687499701976776\n",
      "iter:  323\n",
      "86.34%\n",
      "200/200 [==============================] - 0s 262us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4687499701976776\n",
      "iter:  324\n",
      "86.60%\n",
      "200/200 [==============================] - 0s 184us/step\n",
      "accuracy:  0.38999998569488525\n",
      "precision:  0.38999998569488525\n",
      "recall:  0.38999998569488525\n",
      "f1:  0.401785671710968\n",
      "iter:  325\n",
      "86.87%\n",
      "200/200 [==============================] - 0s 208us/step\n",
      "accuracy:  0.4650000035762787\n",
      "precision:  0.4650000035762787\n",
      "recall:  0.4650000035762787\n",
      "f1:  0.4553571045398712\n",
      "iter:  326\n",
      "87.14%\n",
      "200/200 [==============================] - 0s 617us/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.4553571045398712\n",
      "iter:  327\n",
      "87.41%\n",
      "200/200 [==============================] - 0s 336us/step\n",
      "accuracy:  0.49000000953674316\n",
      "precision:  0.49000000953674316\n",
      "recall:  0.49000000953674316\n",
      "f1:  0.5044642686843872\n",
      "iter:  328\n",
      "87.67%\n",
      "200/200 [==============================] - 0s 292us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.5133928060531616\n",
      "iter:  329\n",
      "87.94%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.4699999988079071\n",
      "precision:  0.4699999988079071\n",
      "recall:  0.4699999988079071\n",
      "f1:  0.4464285373687744\n",
      "iter:  330\n",
      "88.21%\n",
      "200/200 [==============================] - 0s 278us/step\n",
      "accuracy:  0.5149999856948853\n",
      "precision:  0.5149999856948853\n",
      "recall:  0.5149999856948853\n",
      "f1:  0.4999999701976776\n",
      "iter:  331\n",
      "88.47%\n",
      "200/200 [==============================] - 0s 254us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.4866071045398712\n",
      "iter:  332\n",
      "88.74%\n",
      "200/200 [==============================] - 0s 274us/step\n",
      "accuracy:  0.5849999785423279\n",
      "precision:  0.5849999785423279\n",
      "recall:  0.5849999785423279\n",
      "f1:  0.5491071343421936\n",
      "iter:  333\n",
      "89.01%\n",
      "200/200 [==============================] - 0s 373us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5758928656578064\n",
      "iter:  334\n",
      "89.28%\n",
      "200/200 [==============================] - 0s 183us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.4687499701976776\n",
      "iter:  335\n",
      "89.54%\n",
      "200/200 [==============================] - 0s 233us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.482142835855484\n",
      "iter:  336\n",
      "89.81%\n",
      "200/200 [==============================] - 0s 304us/step\n",
      "accuracy:  0.5799999833106995\n",
      "precision:  0.5799999833106995\n",
      "recall:  0.5799999833106995\n",
      "f1:  0.5982142090797424\n",
      "iter:  337\n",
      "90.08%\n",
      "200/200 [==============================] - 0s 188us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5133928060531616\n",
      "iter:  338\n",
      "90.35%\n",
      "200/200 [==============================] - 0s 290us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5401785373687744\n",
      "iter:  339\n",
      "90.61%\n",
      "200/200 [==============================] - 0s 245us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5223214030265808\n",
      "iter:  340\n",
      "90.88%\n",
      "200/200 [==============================] - 0s 236us/step\n",
      "accuracy:  0.5249999761581421\n",
      "precision:  0.5249999761581421\n",
      "recall:  0.5249999761581421\n",
      "f1:  0.5089285373687744\n",
      "iter:  341\n",
      "91.15%\n",
      "200/200 [==============================] - 0s 231us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.464285671710968\n",
      "iter:  342\n",
      "91.41%\n",
      "200/200 [==============================] - 0s 306us/step\n",
      "accuracy:  0.5199999809265137\n",
      "precision:  0.5199999809265137\n",
      "recall:  0.5199999809265137\n",
      "f1:  0.5178571343421936\n",
      "iter:  343\n",
      "91.68%\n",
      "200/200 [==============================] - 0s 210us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5089285373687744\n",
      "iter:  344\n",
      "91.95%\n",
      "200/200 [==============================] - 0s 409us/step\n",
      "accuracy:  0.5699999928474426\n",
      "precision:  0.5699999928474426\n",
      "recall:  0.5699999928474426\n",
      "f1:  0.5357142686843872\n",
      "iter:  345\n",
      "92.22%\n",
      "200/200 [==============================] - 0s 400us/step\n",
      "accuracy:  0.5299999713897705\n",
      "precision:  0.5299999713897705\n",
      "recall:  0.5299999713897705\n",
      "f1:  0.5669642686843872\n",
      "iter:  346\n",
      "92.48%\n",
      "200/200 [==============================] - 0s 214us/step\n",
      "accuracy:  0.550000011920929\n",
      "precision:  0.550000011920929\n",
      "recall:  0.550000011920929\n",
      "f1:  0.4910714030265808\n",
      "iter:  347\n",
      "92.75%\n",
      "200/200 [==============================] - 0s 225us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5669642686843872\n",
      "iter:  348\n",
      "93.02%\n",
      "200/200 [==============================] - 0s 306us/step\n",
      "accuracy:  0.4399999976158142\n",
      "precision:  0.4399999976158142\n",
      "recall:  0.4399999976158142\n",
      "f1:  0.4598214030265808\n",
      "iter:  349\n",
      "93.29%\n",
      "200/200 [==============================] - 0s 230us/step\n",
      "accuracy:  0.45500001311302185\n",
      "precision:  0.45500001311302185\n",
      "recall:  0.45500001311302185\n",
      "f1:  0.4062499701976776\n",
      "iter:  350\n",
      "93.55%\n",
      "200/200 [==============================] - 0s 249us/step\n",
      "accuracy:  0.574999988079071\n",
      "precision:  0.574999988079071\n",
      "recall:  0.574999988079071\n",
      "f1:  0.5535714030265808\n",
      "iter:  351\n",
      "93.82%\n",
      "200/200 [==============================] - 0s 206us/step\n",
      "accuracy:  0.3700000047683716\n",
      "precision:  0.3700000047683716\n",
      "recall:  0.3700000047683716\n",
      "f1:  0.4107142388820648\n",
      "iter:  352\n",
      "94.09%\n",
      "200/200 [==============================] - 0s 500us/step\n",
      "accuracy:  0.4050000011920929\n",
      "precision:  0.4050000011920929\n",
      "recall:  0.4050000011920929\n",
      "f1:  0.4285714030265808\n",
      "iter:  353\n",
      "94.35%\n",
      "200/200 [==============================] - 0s 283us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.5223214030265808\n",
      "iter:  354\n",
      "94.62%\n",
      "200/200 [==============================] - 0s 205us/step\n",
      "accuracy:  0.4300000071525574\n",
      "precision:  0.4300000071525574\n",
      "recall:  0.4300000071525574\n",
      "f1:  0.4107142388820648\n",
      "iter:  355\n",
      "94.89%\n",
      "200/200 [==============================] - 0s 236us/step\n",
      "accuracy:  0.5550000071525574\n",
      "precision:  0.5550000071525574\n",
      "recall:  0.5550000071525574\n",
      "f1:  0.5357142686843872\n",
      "iter:  356\n",
      "95.16%\n",
      "200/200 [==============================] - 0s 179us/step\n",
      "accuracy:  0.5649999976158142\n",
      "precision:  0.5649999976158142\n",
      "recall:  0.5649999976158142\n",
      "f1:  0.5982142090797424\n",
      "iter:  357\n",
      "95.42%\n",
      "200/200 [==============================] - 0s 328us/step\n",
      "accuracy:  0.41999998688697815\n",
      "precision:  0.41999998688697815\n",
      "recall:  0.41999998688697815\n",
      "f1:  0.4419642388820648\n",
      "iter:  358\n",
      "95.69%\n",
      "200/200 [==============================] - 0s 196us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.5223214030265808\n",
      "iter:  359\n",
      "95.96%\n",
      "200/200 [==============================] - 0s 280us/step\n",
      "accuracy:  0.4449999928474426\n",
      "precision:  0.4449999928474426\n",
      "recall:  0.4449999928474426\n",
      "f1:  0.4374999701976776\n",
      "iter:  360\n",
      "96.23%\n",
      "200/200 [==============================] - 0s 291us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.4910714030265808\n",
      "iter:  361\n",
      "96.49%\n",
      "200/200 [==============================] - 0s 214us/step\n",
      "accuracy:  0.5049999952316284\n",
      "precision:  0.5049999952316284\n",
      "recall:  0.5049999952316284\n",
      "f1:  0.5446428060531616\n",
      "iter:  362\n",
      "96.76%\n",
      "200/200 [==============================] - 0s 241us/step\n",
      "accuracy:  0.5600000023841858\n",
      "precision:  0.5600000023841858\n",
      "recall:  0.5600000023841858\n",
      "f1:  0.5535714030265808\n",
      "iter:  363\n",
      "97.03%\n",
      "200/200 [==============================] - 0s 219us/step\n",
      "accuracy:  0.5350000262260437\n",
      "precision:  0.5350000262260437\n",
      "recall:  0.5350000262260437\n",
      "f1:  0.4910714030265808\n",
      "iter:  364\n",
      "97.29%\n",
      "200/200 [==============================] - 0s 198us/step\n",
      "accuracy:  0.47999998927116394\n",
      "precision:  0.47999998927116394\n",
      "recall:  0.47999998927116394\n",
      "f1:  0.4687499701976776\n",
      "iter:  365\n",
      "97.56%\n",
      "200/200 [==============================] - 0s 1ms/step\n",
      "accuracy:  0.4950000047683716\n",
      "precision:  0.4950000047683716\n",
      "recall:  0.4950000047683716\n",
      "f1:  0.482142835855484\n",
      "iter:  366\n",
      "97.83%\n",
      "200/200 [==============================] - 0s 515us/step\n",
      "accuracy:  0.5400000214576721\n",
      "precision:  0.5400000214576721\n",
      "recall:  0.5400000214576721\n",
      "f1:  0.5357142686843872\n",
      "iter:  367\n",
      "98.10%\n",
      "200/200 [==============================] - 0s 296us/step\n",
      "accuracy:  0.5099999904632568\n",
      "precision:  0.5099999904632568\n",
      "recall:  0.5099999904632568\n",
      "f1:  0.495535671710968\n",
      "iter:  368\n",
      "98.36%\n",
      "200/200 [==============================] - 0s 289us/step\n",
      "accuracy:  0.5\n",
      "precision:  0.5\n",
      "recall:  0.5\n",
      "f1:  0.4598214030265808\n",
      "iter:  369\n",
      "98.63%\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST = 200\n",
    "NUM_TRAIN = 1000\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + NUM_TRAIN\n",
    "val_start = train_end\n",
    "val_end = val_start + NUM_TEST\n",
    "\n",
    "step = NUM_TEST\n",
    "\n",
    "accuracy_sum = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "num_iter = 0\n",
    "total_steps = X.shape[0] / step\n",
    "print('total_steps: ', total_steps)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), F1])\n",
    "while val_end <= X.shape[0]:\n",
    "\n",
    "    model.fit(x=X[train_start:train_end], y=y_cat[train_start:train_end], epochs=10, verbose=0, \n",
    "              batch_size=None, validation_data=(X[val_start:val_end], y_cat[val_start:val_end]), \n",
    "              shuffle=False)\n",
    "    _, accuracy, precision, recall, f1 = model.evaluate(x=X[val_start:val_end], y=y_cat[val_start:val_end])\n",
    "    accuracy_sum += accuracy\n",
    "    precision_sum += precision\n",
    "    recall_sum += recall\n",
    "    f1_sum += f1\n",
    "    \n",
    "    print('accuracy: ', accuracy)\n",
    "    print('precision: ', precision)\n",
    "    print('recall: ', recall)\n",
    "    print('f1: ', f1)\n",
    "    \n",
    "    # update\n",
    "    train_start += step\n",
    "    train_end = train_start + NUM_TRAIN\n",
    "    val_start = train_end\n",
    "    val_end = val_start + NUM_TEST\n",
    "    \n",
    "    num_iter += 1\n",
    "    print('iter: ', num_iter)\n",
    "    print('{:.2f}%'.format(100*num_iter/total_steps))\n",
    "    \n",
    "accuracy = accuracy_sum / num_iter\n",
    "precision = precision_sum / num_iter\n",
    "recall = recall_sum / num_iter\n",
    "f1 = f1_sum / num_iter\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4982655814831173"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4982655814831173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4982655814831173"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4968543975818448"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5089677, 0.4910323], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat.sum(axis=0) / y_cat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rint(model.predict(X[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7e4a0d02453f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnum_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnum_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'num_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_cat[-num_test:].sum(axis=0)/y_cat[-num_test:].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
