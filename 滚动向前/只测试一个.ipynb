{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, CuDNNLSTM, Dropout, Conv1D, Conv2D, Reshape, Activation, MaxPooling2D, Flatten,\n",
    "                        BatchNormalization)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import huber_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给ret分类\n",
    "def label_ret(ret):\n",
    "    '''\n",
    "    class 0 = [-inf, -0.1]\n",
    "    class 1 = [-0.1, 0.1] unprofitable\n",
    "    class 2 = [0.1, inf]\n",
    "    '''\n",
    "    label = None\n",
    "    if ret < -0.1:\n",
    "        label = 0\n",
    "    elif -0.1 <= ret and ret <= 0.1:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 2\n",
    "        \n",
    "    return label\n",
    "def label_ret_bi(ret):\n",
    "    label = None\n",
    "    if ret <= 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "def label_ret2(ret):\n",
    "\n",
    "    if -0.1 <= ret and ret <= 0.1:\n",
    "        return 0\n",
    "    elif 0.1 < ret and ret <= 0.3:\n",
    "        return 1\n",
    "    elif 0.3 < ret:\n",
    "        return 2\n",
    "    elif -0.3 <= ret and ret < -0.1:\n",
    "        return 3\n",
    "    elif ret < -0.3:\n",
    "        return 4\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "        \n",
    "def generate_sequence(X_df, y_series, seq_length):\n",
    "    assert (X_df.index == y_series.index).all()\n",
    "    dataX = list()\n",
    "    dataY = list()\n",
    "    index = list()\n",
    "    for i in range(0, X_df.shape[0] - seq_length + 1):\n",
    "        dataX.append(X_df[i:i+seq_length])\n",
    "        dataY.append(y_series[i+seq_length-1])\n",
    "        index.append(y_series.index[i+seq_length-1])\n",
    "        \n",
    "    return dataX, dataY, pd.Index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variety = 'RB'\n",
    "factor_store = pd.HDFStore('/home/data/vb/training_x_150.h5', mode='r')\n",
    "factor_df = factor_store.get(variety)\n",
    "y_store = pd.HDFStore('/home/data/vb/training_y_reg_150.h5', mode='r')\n",
    "y_series = y_store.get(variety)\n",
    "helper_df = pd.read_parquet('/home/data/vb/training_helper_150_{}.parquet'.format(variety))\n",
    "\n",
    "# 对ret做分类\n",
    "ret_y_series = np.exp(y_series) - 1 # 获得回报的原始收益\n",
    "\n",
    "label_y_series = ret_y_series.transform(label_ret_bi).rename('Y_label') # 分类标签\n",
    "ret_label_df = pd.concat([ret_y_series, label_y_series], axis=1) # 合并log ret和label\n",
    "assert (factor_df.index == label_y_series.index).all() # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# 对齐日期 去掉na\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "df = helper_df.join(factor_df, how='inner').join(label_y_series, how='inner')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 再次得到 factor_df, label_y_series, helper_df\n",
    "factor_df = df[factor_df.columns]\n",
    "label_y_series = df[label_y_series.name]\n",
    "helper_df = df[helper_df.columns]\n",
    "\n",
    "assert (factor_df.index == label_y_series.index).all() and \\\n",
    "        (label_y_series.index == helper_df.index).all()     # 确认数据和标签索引一样\n",
    "\n",
    "\n",
    "# normalize data 在这里会丢失dataframe\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(factor_df)\n",
    "factor_normalized = scaler.transform(factor_df)\n",
    "\n",
    "\n",
    "\n",
    "# 将dataframe的index和columns加回去\n",
    "factor_df_normalized = pd.DataFrame(factor_normalized, \n",
    "                                          index=factor_df.index, columns=factor_df.columns)\n",
    "\n",
    "del factor_normalized\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给lstm制造时间序列数据\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "X, y, index = generate_sequence(factor_df_normalized, label_y_series, SEQ_LENGTH)\n",
    "\n",
    "\n",
    "X = np.array([factor_seq_df.values for factor_seq_df in X]) # 将list 转换为ndarray\n",
    "\n",
    "y_cat = keras.utils.to_categorical(y) # 标签转换为one hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu_option():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=config)\n",
    "    keras.backend.tensorflow_backend.set_session(sess)\n",
    "    \n",
    "    return sess\n",
    "    \n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 1\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model_dropout(input_shape, num_classes):\n",
    "    NUM_NEURONS = 1\n",
    "    MULTIPLIER = 8\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(64*MULTIPLIER, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(CuDNNLSTM(32*MULTIPLIER, return_sequences=False))\n",
    "    model.add(Dense(16*MULTIPLIER, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape(input_shape+(1,), input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_conv1d_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=20, kernel_size=30, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=30, kernel_size=30, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def Precision(y_true, y_pred):\n",
    "    \"\"\"精确率\"\"\"\n",
    "    tp= K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))  # true positives\n",
    "    pp= K.sum(K.round(K.clip(y_pred, 0, 1))) # predicted positives\n",
    "    precision = tp/ (pp+ K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def Recall(y_true, y_pred):\n",
    "    \"\"\"召回率\"\"\"\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) # true positives\n",
    "    pp = K.sum(K.round(K.clip(y_true, 0, 1))) # possible positives\n",
    "    recall = tp / (pp + K.epsilon())\n",
    "    return recall\n",
    " \n",
    "def F1(y_true, y_pred):\n",
    "    \"\"\"F1-score\"\"\"\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (100, 176)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 71, 20)            105620    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 71, 20)            80        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 42, 30)            18030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 42, 30)            120       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 13, 30)            27030     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 30)            120       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 390)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              400384    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 557,530\n",
      "Trainable params: 555,322\n",
      "Non-trainable params: 2,208\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3172: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = y_cat.shape[1]\n",
    "sess = set_gpu_option()\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "print('input_shape: ', input_shape)\n",
    "model = create_cnn_conv1d_model(input_shape=input_shape ,num_classes=NUM_CLASSES)\n",
    "adam = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "19000/19000 [==============================] - 7s 394us/step - loss: 0.2513 - accuracy: 0.8941 - precision_1: 0.8941 - recall_1: 0.8941 - F1: 0.8940 - val_loss: 5.5160 - val_accuracy: 0.5400 - val_precision_1: 0.5400 - val_recall_1: 0.5400 - val_F1: 0.5273\n",
      "Epoch 2/20\n",
      "19000/19000 [==============================] - 7s 390us/step - loss: 0.2480 - accuracy: 0.8944 - precision_1: 0.8944 - recall_1: 0.8944 - F1: 0.8944 - val_loss: 5.9254 - val_accuracy: 0.5440 - val_precision_1: 0.5440 - val_recall_1: 0.5440 - val_F1: 0.5312\n",
      "Epoch 3/20\n",
      "19000/19000 [==============================] - 7s 390us/step - loss: 0.2358 - accuracy: 0.8982 - precision_1: 0.8982 - recall_1: 0.8982 - F1: 0.8982 - val_loss: 8.0457 - val_accuracy: 0.5620 - val_precision_1: 0.5620 - val_recall_1: 0.5620 - val_F1: 0.5488\n",
      "Epoch 4/20\n",
      "19000/19000 [==============================] - 8s 395us/step - loss: 0.2393 - accuracy: 0.8985 - precision_1: 0.8985 - recall_1: 0.8985 - F1: 0.8985 - val_loss: 8.0844 - val_accuracy: 0.5270 - val_precision_1: 0.5270 - val_recall_1: 0.5270 - val_F1: 0.5146\n",
      "Epoch 5/20\n",
      "19000/19000 [==============================] - 8s 396us/step - loss: 0.2232 - accuracy: 0.9068 - precision_1: 0.9068 - recall_1: 0.9068 - F1: 0.9068 - val_loss: 8.3083 - val_accuracy: 0.5270 - val_precision_1: 0.5270 - val_recall_1: 0.5270 - val_F1: 0.5146\n",
      "Epoch 6/20\n",
      "19000/19000 [==============================] - 8s 400us/step - loss: 0.2015 - accuracy: 0.9161 - precision_1: 0.9161 - recall_1: 0.9161 - F1: 0.9161 - val_loss: 8.6810 - val_accuracy: 0.5610 - val_precision_1: 0.5610 - val_recall_1: 0.5610 - val_F1: 0.5479\n",
      "Epoch 7/20\n",
      "19000/19000 [==============================] - 7s 388us/step - loss: 0.2172 - accuracy: 0.9098 - precision_1: 0.9098 - recall_1: 0.9098 - F1: 0.9098 - val_loss: 9.4345 - val_accuracy: 0.5560 - val_precision_1: 0.5560 - val_recall_1: 0.5560 - val_F1: 0.5430\n",
      "Epoch 8/20\n",
      "19000/19000 [==============================] - 7s 389us/step - loss: 0.2074 - accuracy: 0.9152 - precision_1: 0.9152 - recall_1: 0.9152 - F1: 0.9152 - val_loss: 9.0326 - val_accuracy: 0.5810 - val_precision_1: 0.5810 - val_recall_1: 0.5810 - val_F1: 0.5674\n",
      "Epoch 9/20\n",
      "19000/19000 [==============================] - 7s 390us/step - loss: 0.1905 - accuracy: 0.9214 - precision_1: 0.9214 - recall_1: 0.9214 - F1: 0.9213 - val_loss: 9.4262 - val_accuracy: 0.5410 - val_precision_1: 0.5410 - val_recall_1: 0.5410 - val_F1: 0.5283\n",
      "Epoch 10/20\n",
      "19000/19000 [==============================] - 7s 392us/step - loss: 0.1963 - accuracy: 0.9205 - precision_1: 0.9205 - recall_1: 0.9205 - F1: 0.9205 - val_loss: 8.5354 - val_accuracy: 0.5170 - val_precision_1: 0.5170 - val_recall_1: 0.5170 - val_F1: 0.5049\n",
      "Epoch 11/20\n",
      "19000/19000 [==============================] - 7s 395us/step - loss: 0.2060 - accuracy: 0.9148 - precision_1: 0.9148 - recall_1: 0.9148 - F1: 0.9148 - val_loss: 12.2537 - val_accuracy: 0.5220 - val_precision_1: 0.5220 - val_recall_1: 0.5220 - val_F1: 0.5098\n",
      "Epoch 12/20\n",
      "19000/19000 [==============================] - 8s 400us/step - loss: 0.1841 - accuracy: 0.9266 - precision_1: 0.9266 - recall_1: 0.9266 - F1: 0.9266 - val_loss: 8.2678 - val_accuracy: 0.5540 - val_precision_1: 0.5540 - val_recall_1: 0.5540 - val_F1: 0.5410\n",
      "Epoch 13/20\n",
      "19000/19000 [==============================] - 7s 390us/step - loss: 0.1588 - accuracy: 0.9366 - precision_1: 0.9366 - recall_1: 0.9366 - F1: 0.9366 - val_loss: 8.5484 - val_accuracy: 0.5400 - val_precision_1: 0.5400 - val_recall_1: 0.5400 - val_F1: 0.5273\n",
      "Epoch 14/20\n",
      "19000/19000 [==============================] - 7s 392us/step - loss: 0.1627 - accuracy: 0.9325 - precision_1: 0.9325 - recall_1: 0.9325 - F1: 0.9325 - val_loss: 12.3357 - val_accuracy: 0.5440 - val_precision_1: 0.5440 - val_recall_1: 0.5440 - val_F1: 0.5312\n",
      "Epoch 15/20\n",
      "19000/19000 [==============================] - 7s 391us/step - loss: 0.1710 - accuracy: 0.9307 - precision_1: 0.9307 - recall_1: 0.9307 - F1: 0.9307 - val_loss: 9.9712 - val_accuracy: 0.5400 - val_precision_1: 0.5400 - val_recall_1: 0.5400 - val_F1: 0.5273\n",
      "Epoch 16/20\n",
      "19000/19000 [==============================] - 7s 392us/step - loss: 0.1858 - accuracy: 0.9236 - precision_1: 0.9236 - recall_1: 0.9236 - F1: 0.9236 - val_loss: 7.8708 - val_accuracy: 0.5240 - val_precision_1: 0.5240 - val_recall_1: 0.5240 - val_F1: 0.5117\n",
      "Epoch 17/20\n",
      "19000/19000 [==============================] - 8s 397us/step - loss: 0.1677 - accuracy: 0.9337 - precision_1: 0.9337 - recall_1: 0.9337 - F1: 0.9337 - val_loss: 10.8782 - val_accuracy: 0.5360 - val_precision_1: 0.5360 - val_recall_1: 0.5360 - val_F1: 0.5234\n",
      "Epoch 18/20\n",
      "19000/19000 [==============================] - 8s 400us/step - loss: 0.1442 - accuracy: 0.9421 - precision_1: 0.9421 - recall_1: 0.9421 - F1: 0.9420 - val_loss: 9.9590 - val_accuracy: 0.5550 - val_precision_1: 0.5550 - val_recall_1: 0.5550 - val_F1: 0.5420\n",
      "Epoch 19/20\n",
      "19000/19000 [==============================] - 7s 392us/step - loss: 0.1514 - accuracy: 0.9395 - precision_1: 0.9395 - recall_1: 0.9395 - F1: 0.9395 - val_loss: 8.0917 - val_accuracy: 0.5470 - val_precision_1: 0.5470 - val_recall_1: 0.5470 - val_F1: 0.5342\n",
      "Epoch 20/20\n",
      "19000/19000 [==============================] - 7s 391us/step - loss: 0.1610 - accuracy: 0.9342 - precision_1: 0.9342 - recall_1: 0.9342 - F1: 0.9342 - val_loss: 7.3286 - val_accuracy: 0.5590 - val_precision_1: 0.5590 - val_recall_1: 0.5590 - val_F1: 0.5459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe23c311a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X[-20000:-1000], y=y_cat[-20000:-1000], epochs=20, verbose=True, \n",
    "              batch_size=None, validation_data=(X[-1000:], y_cat[-1000:]), \n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rint(model.predict(X[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.558, 0.442], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat[-1000:].sum(axis=0)/y_cat[-1000:].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
